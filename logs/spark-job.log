2024-09-18 05:13:00 [main] INFO  HiveConf:187 - Found configuration file file:/home/hive/apache-hive-3.1.3-bin/conf/hive-site.xml
2024-09-18 05:13:01 [main] INFO  SparkContext:60 - Running Spark version 3.5.2
2024-09-18 05:13:01 [main] INFO  SparkContext:60 - OS info Linux, 5.15.49-linuxkit, aarch64
2024-09-18 05:13:01 [main] INFO  SparkContext:60 - Java version 1.8.0_362
2024-09-18 05:13:01 [main] WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-18 05:13:01 [main] INFO  ResourceUtils:60 - ==============================================================
2024-09-18 05:13:01 [main] INFO  ResourceUtils:60 - No custom resources configured for spark.driver.
2024-09-18 05:13:01 [main] INFO  ResourceUtils:60 - ==============================================================
2024-09-18 05:13:01 [main] INFO  SparkContext:60 - Submitted application: CSV to Hive
2024-09-18 05:13:01 [main] INFO  ResourceProfile:60 - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-18 05:13:01 [main] INFO  ResourceProfile:60 - Limiting resource is cpu
2024-09-18 05:13:01 [main] INFO  ResourceProfileManager:60 - Added ResourceProfile id: 0
2024-09-18 05:13:01 [main] INFO  SecurityManager:60 - Changing view acls to: root
2024-09-18 05:13:01 [main] INFO  SecurityManager:60 - Changing modify acls to: root
2024-09-18 05:13:01 [main] INFO  SecurityManager:60 - Changing view acls groups to: 
2024-09-18 05:13:01 [main] INFO  SecurityManager:60 - Changing modify acls groups to: 
2024-09-18 05:13:01 [main] INFO  SecurityManager:60 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
2024-09-18 05:13:01 [main] INFO  Utils:60 - Successfully started service 'sparkDriver' on port 42841.
2024-09-18 05:13:01 [main] INFO  SparkEnv:60 - Registering MapOutputTracker
2024-09-18 05:13:01 [main] INFO  SparkEnv:60 - Registering BlockManagerMaster
2024-09-18 05:13:01 [main] INFO  BlockManagerMasterEndpoint:60 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-18 05:13:01 [main] INFO  BlockManagerMasterEndpoint:60 - BlockManagerMasterEndpoint up
2024-09-18 05:13:01 [main] INFO  SparkEnv:60 - Registering BlockManagerMasterHeartbeat
2024-09-18 05:13:01 [main] INFO  DiskBlockManager:60 - Created local directory at /tmp/blockmgr-3d025210-2c9d-4b56-9bc6-00412bff64bf
2024-09-18 05:13:01 [main] INFO  MemoryStore:60 - MemoryStore started with capacity 1147.2 MiB
2024-09-18 05:13:01 [main] INFO  SparkEnv:60 - Registering OutputCommitCoordinator
2024-09-18 05:13:01 [main] INFO  log:170 - Logging initialized @1509ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-18 05:13:01 [main] INFO  JettyUtils:60 - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-18 05:13:01 [main] INFO  Server:375 - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 1.8.0_362-8u372-ga~us1-0ubuntu1~22.04-b09
2024-09-18 05:13:01 [main] INFO  Server:415 - Started @1596ms
2024-09-18 05:13:01 [main] INFO  AbstractConnector:333 - Started ServerConnector@51827393{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-18 05:13:01 [main] INFO  Utils:60 - Successfully started service 'SparkUI' on port 4040.
2024-09-18 05:13:01 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@213860b8{/,null,AVAILABLE,@Spark}
2024-09-18 05:13:01 [main] INFO  Executor:60 - Starting executor ID driver on host nn
2024-09-18 05:13:01 [main] INFO  Executor:60 - OS info Linux, 5.15.49-linuxkit, aarch64
2024-09-18 05:13:01 [main] INFO  Executor:60 - Java version 1.8.0_362
2024-09-18 05:13:01 [main] INFO  Executor:60 - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-18 05:13:01 [main] INFO  Executor:60 - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@655523dd for default.
2024-09-18 05:13:01 [main] INFO  Utils:60 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39831.
2024-09-18 05:13:01 [main] INFO  NettyBlockTransferService:84 - Server created on nn:39831
2024-09-18 05:13:01 [main] INFO  BlockManager:60 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-18 05:13:01 [main] INFO  BlockManagerMaster:60 - Registering BlockManager BlockManagerId(driver, nn, 39831, None)
2024-09-18 05:13:01 [dispatcher-BlockManagerMaster] INFO  BlockManagerMasterEndpoint:60 - Registering block manager nn:39831 with 1147.2 MiB RAM, BlockManagerId(driver, nn, 39831, None)
2024-09-18 05:13:01 [main] INFO  BlockManagerMaster:60 - Registered BlockManager BlockManagerId(driver, nn, 39831, None)
2024-09-18 05:13:01 [main] INFO  BlockManager:60 - Initialized BlockManager: BlockManagerId(driver, nn, 39831, None)
2024-09-18 05:13:02 [main] INFO  ContextHandler:1159 - Stopped o.s.j.s.ServletContextHandler@213860b8{/,null,STOPPED,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4e17913b{/jobs,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@64f16277{/jobs/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4e6f2bb5{/jobs/job,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3f628ce9{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@26d96e5{/stages,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1846579f{/stages/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@5fac521d{/stages/stage,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@129bd55d{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3abfe845{/stages/pool,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3672276e{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7f08caf{/storage,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2330e3e0{/storage/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@27a2a089{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@706eab5d{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@40e60ece{/environment,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3a230001{/environment/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2aa6311a{/executors,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@249e0271{/executors/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@53a665ad{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@78525ef9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4d654825{/static,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4339e0de{/,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@61d84e08{/api,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@107f4980{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1d540566{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3a01773b{/metrics/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  SharedState:60 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2024-09-18 05:13:02 [main] INFO  SharedState:60 - Warehouse path is 'hdfs://nn:9000/user/hive/warehouse'.
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@32e652b6{/SQL,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7593ea79{/SQL/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@68ee3b6d{/SQL/execution,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2be95d31{/SQL/execution/json,null,AVAILABLE,@Spark}
2024-09-18 05:13:02 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@a5bbaec{/static/sql,null,AVAILABLE,@Spark}
2024-09-18 05:13:03 [main] INFO  InMemoryFileIndex:60 - It took 16 ms to list leaf files for 1 paths.
2024-09-18 05:13:03 [main] INFO  InMemoryFileIndex:60 - It took 0 ms to list leaf files for 1 paths.
2024-09-18 05:13:04 [main] INFO  FileSourceStrategy:60 - Pushed Filters: 
2024-09-18 05:13:04 [main] INFO  FileSourceStrategy:60 - Post-Scan Filters: (length(trim(value#0, None)) > 0)
2024-09-18 05:13:04 [main] INFO  CodeGenerator:60 - Code generated in 115.202167 ms
2024-09-18 05:13:04 [main] INFO  MemoryStore:60 - Block broadcast_0 stored as values in memory (estimated size 352.4 KiB, free 1146.9 MiB)
2024-09-18 05:13:04 [main] INFO  MemoryStore:60 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 1146.8 MiB)
2024-09-18 05:13:04 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_0_piece0 in memory on nn:39831 (size: 34.2 KiB, free: 1147.2 MiB)
2024-09-18 05:13:04 [main] INFO  SparkContext:60 - Created broadcast 0 from load at UserActivityETL.java:56
2024-09-18 05:13:04 [main] INFO  FileSourceScanExec:60 - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2024-09-18 05:13:04 [main] INFO  SparkContext:60 - Starting job: load at UserActivityETL.java:56
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Got job 0 (load at UserActivityETL.java:56) with 1 output partitions
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Final stage: ResultStage 0 (load at UserActivityETL.java:56)
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Parents of final stage: List()
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Missing parents: List()
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting ResultStage 0 (MapPartitionsRDD[3] at load at UserActivityETL.java:56), which has no missing parents
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 1146.8 MiB)
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 1146.8 MiB)
2024-09-18 05:13:04 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_1_piece0 in memory on nn:39831 (size: 6.4 KiB, free: 1147.2 MiB)
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  SparkContext:60 - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at UserActivityETL.java:56) (first 15 tasks are for partitions Vector(0))
2024-09-18 05:13:04 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Adding task set 0.0 with 1 tasks resource profile 0
2024-09-18 05:13:05 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 0.0 in stage 0.0 (TID 0) (nn, executor driver, partition 0, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  Executor:60 - Running task 0.0 in stage 0.0 (TID 0)
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  CodeGenerator:60 - Code generated in 9.142875 ms
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 0-134217728, partition values: [empty row]
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  CodeGenerator:60 - Code generated in 12.359708 ms
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  Executor:60 - Finished task 0.0 in stage 0.0 (TID 0). 1694 bytes result sent to driver
2024-09-18 05:13:05 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 0.0 in stage 0.0 (TID 0) in 263 ms on nn (executor driver) (1/1)
2024-09-18 05:13:05 [task-result-getter-0] INFO  TaskSchedulerImpl:60 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - ResultStage 0 (load at UserActivityETL.java:56) finished in 0.338 s
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Killing all running tasks in stage 0: Stage finished
2024-09-18 05:13:05 [main] INFO  DAGScheduler:60 - Job 0 finished: load at UserActivityETL.java:56, took 0.369039 s
2024-09-18 05:13:05 [main] INFO  CodeGenerator:60 - Code generated in 7.235667 ms
2024-09-18 05:13:05 [main] INFO  FileSourceStrategy:60 - Pushed Filters: 
2024-09-18 05:13:05 [main] INFO  FileSourceStrategy:60 - Post-Scan Filters: 
2024-09-18 05:13:05 [main] INFO  MemoryStore:60 - Block broadcast_2 stored as values in memory (estimated size 352.4 KiB, free 1146.5 MiB)
2024-09-18 05:13:05 [main] INFO  MemoryStore:60 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 1146.4 MiB)
2024-09-18 05:13:05 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_2_piece0 in memory on nn:39831 (size: 34.2 KiB, free: 1147.1 MiB)
2024-09-18 05:13:05 [main] INFO  SparkContext:60 - Created broadcast 2 from load at UserActivityETL.java:56
2024-09-18 05:13:05 [main] INFO  FileSourceScanExec:60 - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2024-09-18 05:13:05 [main] INFO  SparkContext:60 - Starting job: load at UserActivityETL.java:56
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Got job 1 (load at UserActivityETL.java:56) with 43 output partitions
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Final stage: ResultStage 1 (load at UserActivityETL.java:56)
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Parents of final stage: List()
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Missing parents: List()
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting ResultStage 1 (MapPartitionsRDD[9] at load at UserActivityETL.java:56), which has no missing parents
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_3 stored as values in memory (estimated size 27.7 KiB, free 1146.4 MiB)
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 1146.4 MiB)
2024-09-18 05:13:05 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_3_piece0 in memory on nn:39831 (size: 12.7 KiB, free: 1147.1 MiB)
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  SparkContext:60 - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting 43 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at UserActivityETL.java:56) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2024-09-18 05:13:05 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Adding task set 1.0 with 43 tasks resource profile 0
2024-09-18 05:13:05 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 0.0 in stage 1.0 (TID 1) (nn, executor driver, partition 0, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:05 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 1.0 in stage 1.0 (TID 2) (nn, executor driver, partition 1, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:05 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 2.0 in stage 1.0 (TID 3) (nn, executor driver, partition 2, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:05 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 3.0 in stage 1.0 (TID 4) (nn, executor driver, partition 3, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  Executor:60 - Running task 0.0 in stage 1.0 (TID 1)
2024-09-18 05:13:05 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  Executor:60 - Running task 2.0 in stage 1.0 (TID 3)
2024-09-18 05:13:05 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  Executor:60 - Running task 1.0 in stage 1.0 (TID 2)
2024-09-18 05:13:05 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  Executor:60 - Running task 3.0 in stage 1.0 (TID 4)
2024-09-18 05:13:05 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Removed broadcast_1_piece0 on nn:39831 in memory (size: 6.4 KiB, free: 1147.1 MiB)
2024-09-18 05:13:05 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  CodeGenerator:60 - Code generated in 4.568167 ms
2024-09-18 05:13:05 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 134217728-268435456, partition values: [empty row]
2024-09-18 05:13:05 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 268435456-402653184, partition values: [empty row]
2024-09-18 05:13:05 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 0-134217728, partition values: [empty row]
2024-09-18 05:13:05 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 402653184-536870912, partition values: [empty row]
2024-09-18 05:13:08 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  Executor:60 - Finished task 1.0 in stage 1.0 (TID 2). 1843 bytes result sent to driver
2024-09-18 05:13:08 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 4.0 in stage 1.0 (TID 5) (nn, executor driver, partition 4, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:08 [Executor task launch worker for task 4.0 in stage 1.0 (TID 5)] INFO  Executor:60 - Running task 4.0 in stage 1.0 (TID 5)
2024-09-18 05:13:08 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 1.0 in stage 1.0 (TID 2) in 3208 ms on nn (executor driver) (1/43)
2024-09-18 05:13:08 [Executor task launch worker for task 4.0 in stage 1.0 (TID 5)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 536870912-671088640, partition values: [empty row]
2024-09-18 05:13:08 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  Executor:60 - Finished task 3.0 in stage 1.0 (TID 4). 1800 bytes result sent to driver
2024-09-18 05:13:08 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 5.0 in stage 1.0 (TID 6) (nn, executor driver, partition 5, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:08 [Executor task launch worker for task 5.0 in stage 1.0 (TID 6)] INFO  Executor:60 - Running task 5.0 in stage 1.0 (TID 6)
2024-09-18 05:13:08 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 3.0 in stage 1.0 (TID 4) in 3224 ms on nn (executor driver) (2/43)
2024-09-18 05:13:08 [Executor task launch worker for task 5.0 in stage 1.0 (TID 6)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 671088640-805306368, partition values: [empty row]
2024-09-18 05:13:08 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  Executor:60 - Finished task 0.0 in stage 1.0 (TID 1). 1800 bytes result sent to driver
2024-09-18 05:13:08 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 6.0 in stage 1.0 (TID 7) (nn, executor driver, partition 6, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:08 [Executor task launch worker for task 6.0 in stage 1.0 (TID 7)] INFO  Executor:60 - Running task 6.0 in stage 1.0 (TID 7)
2024-09-18 05:13:08 [Executor task launch worker for task 6.0 in stage 1.0 (TID 7)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 805306368-939524096, partition values: [empty row]
2024-09-18 05:13:08 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 0.0 in stage 1.0 (TID 1) in 3344 ms on nn (executor driver) (3/43)
2024-09-18 05:13:08 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  Executor:60 - Finished task 2.0 in stage 1.0 (TID 3). 1800 bytes result sent to driver
2024-09-18 05:13:08 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 7.0 in stage 1.0 (TID 8) (nn, executor driver, partition 7, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:08 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 2.0 in stage 1.0 (TID 3) in 3500 ms on nn (executor driver) (4/43)
2024-09-18 05:13:08 [Executor task launch worker for task 7.0 in stage 1.0 (TID 8)] INFO  Executor:60 - Running task 7.0 in stage 1.0 (TID 8)
2024-09-18 05:13:08 [Executor task launch worker for task 7.0 in stage 1.0 (TID 8)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 939524096-1073741824, partition values: [empty row]
2024-09-18 05:13:11 [Executor task launch worker for task 5.0 in stage 1.0 (TID 6)] INFO  Executor:60 - Finished task 5.0 in stage 1.0 (TID 6). 1800 bytes result sent to driver
2024-09-18 05:13:11 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 8.0 in stage 1.0 (TID 9) (nn, executor driver, partition 8, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:11 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 5.0 in stage 1.0 (TID 6) in 2886 ms on nn (executor driver) (5/43)
2024-09-18 05:13:11 [Executor task launch worker for task 8.0 in stage 1.0 (TID 9)] INFO  Executor:60 - Running task 8.0 in stage 1.0 (TID 9)
2024-09-18 05:13:11 [Executor task launch worker for task 4.0 in stage 1.0 (TID 5)] INFO  Executor:60 - Finished task 4.0 in stage 1.0 (TID 5). 1800 bytes result sent to driver
2024-09-18 05:13:11 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 9.0 in stage 1.0 (TID 10) (nn, executor driver, partition 9, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:11 [Executor task launch worker for task 9.0 in stage 1.0 (TID 10)] INFO  Executor:60 - Running task 9.0 in stage 1.0 (TID 10)
2024-09-18 05:13:11 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 4.0 in stage 1.0 (TID 5) in 2924 ms on nn (executor driver) (6/43)
2024-09-18 05:13:11 [Executor task launch worker for task 9.0 in stage 1.0 (TID 10)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1207959552-1342177280, partition values: [empty row]
2024-09-18 05:13:11 [Executor task launch worker for task 6.0 in stage 1.0 (TID 7)] INFO  Executor:60 - Finished task 6.0 in stage 1.0 (TID 7). 1800 bytes result sent to driver
2024-09-18 05:13:11 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 10.0 in stage 1.0 (TID 11) (nn, executor driver, partition 10, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:11 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 6.0 in stage 1.0 (TID 7) in 2804 ms on nn (executor driver) (7/43)
2024-09-18 05:13:11 [Executor task launch worker for task 10.0 in stage 1.0 (TID 11)] INFO  Executor:60 - Running task 10.0 in stage 1.0 (TID 11)
2024-09-18 05:13:11 [Executor task launch worker for task 10.0 in stage 1.0 (TID 11)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1342177280-1476395008, partition values: [empty row]
2024-09-18 05:13:11 [Executor task launch worker for task 8.0 in stage 1.0 (TID 9)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1073741824-1207959552, partition values: [empty row]
2024-09-18 05:13:11 [Executor task launch worker for task 7.0 in stage 1.0 (TID 8)] INFO  Executor:60 - Finished task 7.0 in stage 1.0 (TID 8). 1800 bytes result sent to driver
2024-09-18 05:13:11 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 11.0 in stage 1.0 (TID 12) (nn, executor driver, partition 11, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:11 [Executor task launch worker for task 11.0 in stage 1.0 (TID 12)] INFO  Executor:60 - Running task 11.0 in stage 1.0 (TID 12)
2024-09-18 05:13:11 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 7.0 in stage 1.0 (TID 8) in 2766 ms on nn (executor driver) (8/43)
2024-09-18 05:13:11 [Executor task launch worker for task 11.0 in stage 1.0 (TID 12)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1476395008-1610612736, partition values: [empty row]
2024-09-18 05:13:14 [Executor task launch worker for task 8.0 in stage 1.0 (TID 9)] INFO  Executor:60 - Finished task 8.0 in stage 1.0 (TID 9). 1800 bytes result sent to driver
2024-09-18 05:13:14 [Executor task launch worker for task 9.0 in stage 1.0 (TID 10)] INFO  Executor:60 - Finished task 9.0 in stage 1.0 (TID 10). 1800 bytes result sent to driver
2024-09-18 05:13:14 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 12.0 in stage 1.0 (TID 13) (nn, executor driver, partition 12, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:14 [Executor task launch worker for task 12.0 in stage 1.0 (TID 13)] INFO  Executor:60 - Running task 12.0 in stage 1.0 (TID 13)
2024-09-18 05:13:14 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 13.0 in stage 1.0 (TID 14) (nn, executor driver, partition 13, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:14 [Executor task launch worker for task 13.0 in stage 1.0 (TID 14)] INFO  Executor:60 - Running task 13.0 in stage 1.0 (TID 14)
2024-09-18 05:13:14 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 9.0 in stage 1.0 (TID 10) in 2506 ms on nn (executor driver) (9/43)
2024-09-18 05:13:14 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 8.0 in stage 1.0 (TID 9) in 2528 ms on nn (executor driver) (10/43)
2024-09-18 05:13:14 [Executor task launch worker for task 13.0 in stage 1.0 (TID 14)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1744830464-1879048192, partition values: [empty row]
2024-09-18 05:13:14 [Executor task launch worker for task 12.0 in stage 1.0 (TID 13)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1610612736-1744830464, partition values: [empty row]
2024-09-18 05:13:14 [Executor task launch worker for task 10.0 in stage 1.0 (TID 11)] INFO  Executor:60 - Finished task 10.0 in stage 1.0 (TID 11). 1800 bytes result sent to driver
2024-09-18 05:13:14 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 14.0 in stage 1.0 (TID 15) (nn, executor driver, partition 14, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:14 [Executor task launch worker for task 14.0 in stage 1.0 (TID 15)] INFO  Executor:60 - Running task 14.0 in stage 1.0 (TID 15)
2024-09-18 05:13:14 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 10.0 in stage 1.0 (TID 11) in 2568 ms on nn (executor driver) (11/43)
2024-09-18 05:13:14 [Executor task launch worker for task 14.0 in stage 1.0 (TID 15)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1879048192-2013265920, partition values: [empty row]
2024-09-18 05:13:14 [Executor task launch worker for task 11.0 in stage 1.0 (TID 12)] INFO  Executor:60 - Finished task 11.0 in stage 1.0 (TID 12). 1800 bytes result sent to driver
2024-09-18 05:13:14 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 15.0 in stage 1.0 (TID 16) (nn, executor driver, partition 15, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:14 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 11.0 in stage 1.0 (TID 12) in 2702 ms on nn (executor driver) (12/43)
2024-09-18 05:13:14 [Executor task launch worker for task 15.0 in stage 1.0 (TID 16)] INFO  Executor:60 - Running task 15.0 in stage 1.0 (TID 16)
2024-09-18 05:13:14 [Executor task launch worker for task 15.0 in stage 1.0 (TID 16)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2013265920-2147483648, partition values: [empty row]
2024-09-18 05:13:16 [Executor task launch worker for task 12.0 in stage 1.0 (TID 13)] INFO  Executor:60 - Finished task 12.0 in stage 1.0 (TID 13). 1843 bytes result sent to driver
2024-09-18 05:13:16 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 16.0 in stage 1.0 (TID 17) (nn, executor driver, partition 16, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:16 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 12.0 in stage 1.0 (TID 13) in 2841 ms on nn (executor driver) (13/43)
2024-09-18 05:13:16 [Executor task launch worker for task 16.0 in stage 1.0 (TID 17)] INFO  Executor:60 - Running task 16.0 in stage 1.0 (TID 17)
2024-09-18 05:13:16 [Executor task launch worker for task 16.0 in stage 1.0 (TID 17)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2147483648-2281701376, partition values: [empty row]
2024-09-18 05:13:17 [Executor task launch worker for task 13.0 in stage 1.0 (TID 14)] INFO  Executor:60 - Finished task 13.0 in stage 1.0 (TID 14). 1800 bytes result sent to driver
2024-09-18 05:13:17 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 17.0 in stage 1.0 (TID 18) (nn, executor driver, partition 17, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:17 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 13.0 in stage 1.0 (TID 14) in 2957 ms on nn (executor driver) (14/43)
2024-09-18 05:13:17 [Executor task launch worker for task 17.0 in stage 1.0 (TID 18)] INFO  Executor:60 - Running task 17.0 in stage 1.0 (TID 18)
2024-09-18 05:13:17 [Executor task launch worker for task 17.0 in stage 1.0 (TID 18)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2281701376-2415919104, partition values: [empty row]
2024-09-18 05:13:17 [Executor task launch worker for task 14.0 in stage 1.0 (TID 15)] INFO  Executor:60 - Finished task 14.0 in stage 1.0 (TID 15). 1800 bytes result sent to driver
2024-09-18 05:13:17 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 18.0 in stage 1.0 (TID 19) (nn, executor driver, partition 18, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:17 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 14.0 in stage 1.0 (TID 15) in 2902 ms on nn (executor driver) (15/43)
2024-09-18 05:13:17 [Executor task launch worker for task 18.0 in stage 1.0 (TID 19)] INFO  Executor:60 - Running task 18.0 in stage 1.0 (TID 19)
2024-09-18 05:13:17 [Executor task launch worker for task 18.0 in stage 1.0 (TID 19)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2415919104-2550136832, partition values: [empty row]
2024-09-18 05:13:17 [Executor task launch worker for task 15.0 in stage 1.0 (TID 16)] INFO  Executor:60 - Finished task 15.0 in stage 1.0 (TID 16). 1843 bytes result sent to driver
2024-09-18 05:13:17 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 19.0 in stage 1.0 (TID 20) (nn, executor driver, partition 19, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:17 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 15.0 in stage 1.0 (TID 16) in 2760 ms on nn (executor driver) (16/43)
2024-09-18 05:13:17 [Executor task launch worker for task 19.0 in stage 1.0 (TID 20)] INFO  Executor:60 - Running task 19.0 in stage 1.0 (TID 20)
2024-09-18 05:13:17 [Executor task launch worker for task 19.0 in stage 1.0 (TID 20)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2550136832-2684354560, partition values: [empty row]
2024-09-18 05:13:19 [Executor task launch worker for task 16.0 in stage 1.0 (TID 17)] INFO  Executor:60 - Finished task 16.0 in stage 1.0 (TID 17). 1800 bytes result sent to driver
2024-09-18 05:13:19 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 20.0 in stage 1.0 (TID 21) (nn, executor driver, partition 20, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:19 [Executor task launch worker for task 20.0 in stage 1.0 (TID 21)] INFO  Executor:60 - Running task 20.0 in stage 1.0 (TID 21)
2024-09-18 05:13:19 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 16.0 in stage 1.0 (TID 17) in 3033 ms on nn (executor driver) (17/43)
2024-09-18 05:13:19 [Executor task launch worker for task 20.0 in stage 1.0 (TID 21)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2684354560-2818572288, partition values: [empty row]
2024-09-18 05:13:20 [Executor task launch worker for task 18.0 in stage 1.0 (TID 19)] INFO  Executor:60 - Finished task 18.0 in stage 1.0 (TID 19). 1800 bytes result sent to driver
2024-09-18 05:13:20 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 21.0 in stage 1.0 (TID 22) (nn, executor driver, partition 21, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:20 [Executor task launch worker for task 21.0 in stage 1.0 (TID 22)] INFO  Executor:60 - Running task 21.0 in stage 1.0 (TID 22)
2024-09-18 05:13:20 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 18.0 in stage 1.0 (TID 19) in 3083 ms on nn (executor driver) (18/43)
2024-09-18 05:13:20 [Executor task launch worker for task 21.0 in stage 1.0 (TID 22)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2818572288-2952790016, partition values: [empty row]
2024-09-18 05:13:20 [Executor task launch worker for task 17.0 in stage 1.0 (TID 18)] INFO  Executor:60 - Finished task 17.0 in stage 1.0 (TID 18). 1800 bytes result sent to driver
2024-09-18 05:13:20 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 22.0 in stage 1.0 (TID 23) (nn, executor driver, partition 22, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:20 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 17.0 in stage 1.0 (TID 18) in 3206 ms on nn (executor driver) (19/43)
2024-09-18 05:13:20 [Executor task launch worker for task 22.0 in stage 1.0 (TID 23)] INFO  Executor:60 - Running task 22.0 in stage 1.0 (TID 23)
2024-09-18 05:13:20 [Executor task launch worker for task 22.0 in stage 1.0 (TID 23)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2952790016-3087007744, partition values: [empty row]
2024-09-18 05:13:20 [Executor task launch worker for task 19.0 in stage 1.0 (TID 20)] INFO  Executor:60 - Finished task 19.0 in stage 1.0 (TID 20). 1800 bytes result sent to driver
2024-09-18 05:13:20 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 23.0 in stage 1.0 (TID 24) (nn, executor driver, partition 23, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:20 [Executor task launch worker for task 23.0 in stage 1.0 (TID 24)] INFO  Executor:60 - Running task 23.0 in stage 1.0 (TID 24)
2024-09-18 05:13:20 [Executor task launch worker for task 23.0 in stage 1.0 (TID 24)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3087007744-3221225472, partition values: [empty row]
2024-09-18 05:13:20 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 19.0 in stage 1.0 (TID 20) in 3152 ms on nn (executor driver) (20/43)
2024-09-18 05:13:23 [Executor task launch worker for task 22.0 in stage 1.0 (TID 23)] INFO  Executor:60 - Finished task 22.0 in stage 1.0 (TID 23). 1800 bytes result sent to driver
2024-09-18 05:13:23 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 24.0 in stage 1.0 (TID 25) (nn, executor driver, partition 24, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:23 [Executor task launch worker for task 24.0 in stage 1.0 (TID 25)] INFO  Executor:60 - Running task 24.0 in stage 1.0 (TID 25)
2024-09-18 05:13:23 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 22.0 in stage 1.0 (TID 23) in 2790 ms on nn (executor driver) (21/43)
2024-09-18 05:13:23 [Executor task launch worker for task 24.0 in stage 1.0 (TID 25)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3221225472-3355443200, partition values: [empty row]
2024-09-18 05:13:23 [Executor task launch worker for task 21.0 in stage 1.0 (TID 22)] INFO  Executor:60 - Finished task 21.0 in stage 1.0 (TID 22). 1800 bytes result sent to driver
2024-09-18 05:13:23 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 25.0 in stage 1.0 (TID 26) (nn, executor driver, partition 25, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:23 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 21.0 in stage 1.0 (TID 22) in 2988 ms on nn (executor driver) (22/43)
2024-09-18 05:13:23 [Executor task launch worker for task 20.0 in stage 1.0 (TID 21)] INFO  Executor:60 - Finished task 20.0 in stage 1.0 (TID 21). 1800 bytes result sent to driver
2024-09-18 05:13:23 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 26.0 in stage 1.0 (TID 27) (nn, executor driver, partition 26, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:23 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 20.0 in stage 1.0 (TID 21) in 3215 ms on nn (executor driver) (23/43)
2024-09-18 05:13:23 [Executor task launch worker for task 25.0 in stage 1.0 (TID 26)] INFO  Executor:60 - Running task 25.0 in stage 1.0 (TID 26)
2024-09-18 05:13:23 [Executor task launch worker for task 26.0 in stage 1.0 (TID 27)] INFO  Executor:60 - Running task 26.0 in stage 1.0 (TID 27)
2024-09-18 05:13:23 [Executor task launch worker for task 25.0 in stage 1.0 (TID 26)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3355443200-3489660928, partition values: [empty row]
2024-09-18 05:13:23 [Executor task launch worker for task 26.0 in stage 1.0 (TID 27)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3489660928-3623878656, partition values: [empty row]
2024-09-18 05:13:23 [Executor task launch worker for task 23.0 in stage 1.0 (TID 24)] INFO  Executor:60 - Finished task 23.0 in stage 1.0 (TID 24). 1800 bytes result sent to driver
2024-09-18 05:13:23 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 27.0 in stage 1.0 (TID 28) (nn, executor driver, partition 27, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:23 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 23.0 in stage 1.0 (TID 24) in 3072 ms on nn (executor driver) (24/43)
2024-09-18 05:13:23 [Executor task launch worker for task 27.0 in stage 1.0 (TID 28)] INFO  Executor:60 - Running task 27.0 in stage 1.0 (TID 28)
2024-09-18 05:13:23 [Executor task launch worker for task 27.0 in stage 1.0 (TID 28)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3623878656-3758096384, partition values: [empty row]
2024-09-18 05:13:26 [Executor task launch worker for task 24.0 in stage 1.0 (TID 25)] INFO  Executor:60 - Finished task 24.0 in stage 1.0 (TID 25). 1843 bytes result sent to driver
2024-09-18 05:13:26 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 28.0 in stage 1.0 (TID 29) (nn, executor driver, partition 28, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:26 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 24.0 in stage 1.0 (TID 25) in 3252 ms on nn (executor driver) (25/43)
2024-09-18 05:13:26 [Executor task launch worker for task 28.0 in stage 1.0 (TID 29)] INFO  Executor:60 - Running task 28.0 in stage 1.0 (TID 29)
2024-09-18 05:13:26 [Executor task launch worker for task 28.0 in stage 1.0 (TID 29)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3758096384-3892314112, partition values: [empty row]
2024-09-18 05:13:26 [Executor task launch worker for task 25.0 in stage 1.0 (TID 26)] INFO  Executor:60 - Finished task 25.0 in stage 1.0 (TID 26). 1800 bytes result sent to driver
2024-09-18 05:13:26 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 29.0 in stage 1.0 (TID 30) (nn, executor driver, partition 29, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:26 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 25.0 in stage 1.0 (TID 26) in 3241 ms on nn (executor driver) (26/43)
2024-09-18 05:13:26 [Executor task launch worker for task 29.0 in stage 1.0 (TID 30)] INFO  Executor:60 - Running task 29.0 in stage 1.0 (TID 30)
2024-09-18 05:13:26 [Executor task launch worker for task 29.0 in stage 1.0 (TID 30)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3892314112-4026531840, partition values: [empty row]
2024-09-18 05:13:26 [Executor task launch worker for task 26.0 in stage 1.0 (TID 27)] INFO  Executor:60 - Finished task 26.0 in stage 1.0 (TID 27). 1800 bytes result sent to driver
2024-09-18 05:13:26 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 30.0 in stage 1.0 (TID 31) (nn, executor driver, partition 30, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:26 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 26.0 in stage 1.0 (TID 27) in 3469 ms on nn (executor driver) (27/43)
2024-09-18 05:13:26 [Executor task launch worker for task 30.0 in stage 1.0 (TID 31)] INFO  Executor:60 - Running task 30.0 in stage 1.0 (TID 31)
2024-09-18 05:13:26 [Executor task launch worker for task 30.0 in stage 1.0 (TID 31)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4026531840-4160749568, partition values: [empty row]
2024-09-18 05:13:26 [Executor task launch worker for task 27.0 in stage 1.0 (TID 28)] INFO  Executor:60 - Finished task 27.0 in stage 1.0 (TID 28). 1800 bytes result sent to driver
2024-09-18 05:13:26 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 31.0 in stage 1.0 (TID 32) (nn, executor driver, partition 31, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:26 [Executor task launch worker for task 31.0 in stage 1.0 (TID 32)] INFO  Executor:60 - Running task 31.0 in stage 1.0 (TID 32)
2024-09-18 05:13:26 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 27.0 in stage 1.0 (TID 28) in 3267 ms on nn (executor driver) (28/43)
2024-09-18 05:13:26 [Executor task launch worker for task 31.0 in stage 1.0 (TID 32)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4160749568-4294967296, partition values: [empty row]
2024-09-18 05:13:28 [Executor task launch worker for task 28.0 in stage 1.0 (TID 29)] INFO  Executor:60 - Finished task 28.0 in stage 1.0 (TID 29). 1800 bytes result sent to driver
2024-09-18 05:13:28 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 32.0 in stage 1.0 (TID 33) (nn, executor driver, partition 32, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:28 [Executor task launch worker for task 32.0 in stage 1.0 (TID 33)] INFO  Executor:60 - Running task 32.0 in stage 1.0 (TID 33)
2024-09-18 05:13:28 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 28.0 in stage 1.0 (TID 29) in 2618 ms on nn (executor driver) (29/43)
2024-09-18 05:13:28 [Executor task launch worker for task 32.0 in stage 1.0 (TID 33)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4294967296-4429185024, partition values: [empty row]
2024-09-18 05:13:29 [Executor task launch worker for task 29.0 in stage 1.0 (TID 30)] INFO  Executor:60 - Finished task 29.0 in stage 1.0 (TID 30). 1800 bytes result sent to driver
2024-09-18 05:13:29 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 33.0 in stage 1.0 (TID 34) (nn, executor driver, partition 33, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:29 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 29.0 in stage 1.0 (TID 30) in 2790 ms on nn (executor driver) (30/43)
2024-09-18 05:13:29 [Executor task launch worker for task 33.0 in stage 1.0 (TID 34)] INFO  Executor:60 - Running task 33.0 in stage 1.0 (TID 34)
2024-09-18 05:13:29 [Executor task launch worker for task 33.0 in stage 1.0 (TID 34)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4429185024-4563402752, partition values: [empty row]
2024-09-18 05:13:29 [Executor task launch worker for task 30.0 in stage 1.0 (TID 31)] INFO  Executor:60 - Finished task 30.0 in stage 1.0 (TID 31). 1800 bytes result sent to driver
2024-09-18 05:13:29 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 34.0 in stage 1.0 (TID 35) (nn, executor driver, partition 34, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:29 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 30.0 in stage 1.0 (TID 31) in 2632 ms on nn (executor driver) (31/43)
2024-09-18 05:13:29 [Executor task launch worker for task 34.0 in stage 1.0 (TID 35)] INFO  Executor:60 - Running task 34.0 in stage 1.0 (TID 35)
2024-09-18 05:13:29 [Executor task launch worker for task 34.0 in stage 1.0 (TID 35)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4563402752-4697620480, partition values: [empty row]
2024-09-18 05:13:29 [Executor task launch worker for task 31.0 in stage 1.0 (TID 32)] INFO  Executor:60 - Finished task 31.0 in stage 1.0 (TID 32). 1800 bytes result sent to driver
2024-09-18 05:13:29 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 35.0 in stage 1.0 (TID 36) (nn, executor driver, partition 35, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:29 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 31.0 in stage 1.0 (TID 32) in 2999 ms on nn (executor driver) (32/43)
2024-09-18 05:13:29 [Executor task launch worker for task 35.0 in stage 1.0 (TID 36)] INFO  Executor:60 - Running task 35.0 in stage 1.0 (TID 36)
2024-09-18 05:13:29 [Executor task launch worker for task 35.0 in stage 1.0 (TID 36)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4697620480-4831838208, partition values: [empty row]
2024-09-18 05:13:32 [Executor task launch worker for task 32.0 in stage 1.0 (TID 33)] INFO  Executor:60 - Finished task 32.0 in stage 1.0 (TID 33). 1843 bytes result sent to driver
2024-09-18 05:13:32 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 36.0 in stage 1.0 (TID 37) (nn, executor driver, partition 36, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:32 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 32.0 in stage 1.0 (TID 33) in 3402 ms on nn (executor driver) (33/43)
2024-09-18 05:13:32 [Executor task launch worker for task 36.0 in stage 1.0 (TID 37)] INFO  Executor:60 - Running task 36.0 in stage 1.0 (TID 37)
2024-09-18 05:13:32 [Executor task launch worker for task 36.0 in stage 1.0 (TID 37)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4831838208-4966055936, partition values: [empty row]
2024-09-18 05:13:32 [Executor task launch worker for task 33.0 in stage 1.0 (TID 34)] INFO  Executor:60 - Finished task 33.0 in stage 1.0 (TID 34). 1800 bytes result sent to driver
2024-09-18 05:13:32 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 37.0 in stage 1.0 (TID 38) (nn, executor driver, partition 37, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:32 [Executor task launch worker for task 37.0 in stage 1.0 (TID 38)] INFO  Executor:60 - Running task 37.0 in stage 1.0 (TID 38)
2024-09-18 05:13:32 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 33.0 in stage 1.0 (TID 34) in 3279 ms on nn (executor driver) (34/43)
2024-09-18 05:13:32 [Executor task launch worker for task 37.0 in stage 1.0 (TID 38)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4966055936-5100273664, partition values: [empty row]
2024-09-18 05:13:32 [Executor task launch worker for task 34.0 in stage 1.0 (TID 35)] INFO  Executor:60 - Finished task 34.0 in stage 1.0 (TID 35). 1800 bytes result sent to driver
2024-09-18 05:13:32 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 38.0 in stage 1.0 (TID 39) (nn, executor driver, partition 38, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:32 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 34.0 in stage 1.0 (TID 35) in 3323 ms on nn (executor driver) (35/43)
2024-09-18 05:13:32 [Executor task launch worker for task 38.0 in stage 1.0 (TID 39)] INFO  Executor:60 - Running task 38.0 in stage 1.0 (TID 39)
2024-09-18 05:13:32 [Executor task launch worker for task 38.0 in stage 1.0 (TID 39)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5100273664-5234491392, partition values: [empty row]
2024-09-18 05:13:33 [Executor task launch worker for task 35.0 in stage 1.0 (TID 36)] INFO  Executor:60 - Finished task 35.0 in stage 1.0 (TID 36). 1800 bytes result sent to driver
2024-09-18 05:13:33 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 39.0 in stage 1.0 (TID 40) (nn, executor driver, partition 39, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:33 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 35.0 in stage 1.0 (TID 36) in 4137 ms on nn (executor driver) (36/43)
2024-09-18 05:13:33 [Executor task launch worker for task 39.0 in stage 1.0 (TID 40)] INFO  Executor:60 - Running task 39.0 in stage 1.0 (TID 40)
2024-09-18 05:13:33 [Executor task launch worker for task 39.0 in stage 1.0 (TID 40)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5234491392-5368709120, partition values: [empty row]
2024-09-18 05:13:35 [Executor task launch worker for task 36.0 in stage 1.0 (TID 37)] INFO  Executor:60 - Finished task 36.0 in stage 1.0 (TID 37). 1800 bytes result sent to driver
2024-09-18 05:13:35 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 40.0 in stage 1.0 (TID 41) (nn, executor driver, partition 40, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:35 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 36.0 in stage 1.0 (TID 37) in 3039 ms on nn (executor driver) (37/43)
2024-09-18 05:13:35 [Executor task launch worker for task 40.0 in stage 1.0 (TID 41)] INFO  Executor:60 - Running task 40.0 in stage 1.0 (TID 41)
2024-09-18 05:13:35 [Executor task launch worker for task 40.0 in stage 1.0 (TID 41)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5368709120-5502926848, partition values: [empty row]
2024-09-18 05:13:35 [Executor task launch worker for task 37.0 in stage 1.0 (TID 38)] INFO  Executor:60 - Finished task 37.0 in stage 1.0 (TID 38). 1800 bytes result sent to driver
2024-09-18 05:13:35 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 41.0 in stage 1.0 (TID 42) (nn, executor driver, partition 41, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:35 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 37.0 in stage 1.0 (TID 38) in 3091 ms on nn (executor driver) (38/43)
2024-09-18 05:13:35 [Executor task launch worker for task 41.0 in stage 1.0 (TID 42)] INFO  Executor:60 - Running task 41.0 in stage 1.0 (TID 42)
2024-09-18 05:13:35 [Executor task launch worker for task 41.0 in stage 1.0 (TID 42)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5502926848-5637144576, partition values: [empty row]
2024-09-18 05:13:35 [Executor task launch worker for task 38.0 in stage 1.0 (TID 39)] INFO  Executor:60 - Finished task 38.0 in stage 1.0 (TID 39). 1800 bytes result sent to driver
2024-09-18 05:13:35 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 42.0 in stage 1.0 (TID 43) (nn, executor driver, partition 42, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:13:35 [Executor task launch worker for task 42.0 in stage 1.0 (TID 43)] INFO  Executor:60 - Running task 42.0 in stage 1.0 (TID 43)
2024-09-18 05:13:35 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 38.0 in stage 1.0 (TID 39) in 2995 ms on nn (executor driver) (39/43)
2024-09-18 05:13:35 [Executor task launch worker for task 42.0 in stage 1.0 (TID 43)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5637144576-5668612855, partition values: [empty row]
2024-09-18 05:13:36 [Executor task launch worker for task 42.0 in stage 1.0 (TID 43)] INFO  Executor:60 - Finished task 42.0 in stage 1.0 (TID 43). 1800 bytes result sent to driver
2024-09-18 05:13:36 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 42.0 in stage 1.0 (TID 43) in 924 ms on nn (executor driver) (40/43)
2024-09-18 05:13:36 [Executor task launch worker for task 39.0 in stage 1.0 (TID 40)] INFO  Executor:60 - Finished task 39.0 in stage 1.0 (TID 40). 1800 bytes result sent to driver
2024-09-18 05:13:36 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 39.0 in stage 1.0 (TID 40) in 2951 ms on nn (executor driver) (41/43)
2024-09-18 05:13:37 [Executor task launch worker for task 40.0 in stage 1.0 (TID 41)] INFO  Executor:60 - Finished task 40.0 in stage 1.0 (TID 41). 1800 bytes result sent to driver
2024-09-18 05:13:37 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 40.0 in stage 1.0 (TID 41) in 2656 ms on nn (executor driver) (42/43)
2024-09-18 05:13:38 [Executor task launch worker for task 41.0 in stage 1.0 (TID 42)] INFO  Executor:60 - Finished task 41.0 in stage 1.0 (TID 42). 1800 bytes result sent to driver
2024-09-18 05:13:38 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 41.0 in stage 1.0 (TID 42) in 2575 ms on nn (executor driver) (43/43)
2024-09-18 05:13:38 [task-result-getter-3] INFO  TaskSchedulerImpl:60 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2024-09-18 05:13:38 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - ResultStage 1 (load at UserActivityETL.java:56) finished in 32.678 s
2024-09-18 05:13:38 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2024-09-18 05:13:38 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Killing all running tasks in stage 1: Stage finished
2024-09-18 05:13:38 [main] INFO  DAGScheduler:60 - Job 1 finished: load at UserActivityETL.java:56, took 32.687519 s
2024-09-18 05:13:38 [main] INFO  SparkContext:60 - SparkContext is stopping with exitCode 0.
2024-09-18 05:13:38 [main] INFO  AbstractConnector:383 - Stopped Spark@51827393{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-18 05:13:38 [main] INFO  SparkUI:60 - Stopped Spark web UI at http://nn:4040
2024-09-18 05:13:38 [dispatcher-event-loop-0] INFO  MapOutputTrackerMasterEndpoint:60 - MapOutputTrackerMasterEndpoint stopped!
2024-09-18 05:13:38 [main] INFO  MemoryStore:60 - MemoryStore cleared
2024-09-18 05:13:38 [main] INFO  BlockManager:60 - BlockManager stopped
2024-09-18 05:13:38 [main] INFO  BlockManagerMaster:60 - BlockManagerMaster stopped
2024-09-18 05:13:38 [dispatcher-event-loop-1] INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:60 - OutputCommitCoordinator stopped!
2024-09-18 05:13:38 [main] INFO  SparkContext:60 - Successfully stopped SparkContext
2024-09-18 05:13:38 [main] INFO  UserActivityETL:101 - spark job finished.
2024-09-18 05:13:38 [shutdown-hook-0] INFO  ShutdownHookManager:60 - Shutdown hook called
2024-09-18 05:13:38 [shutdown-hook-0] INFO  ShutdownHookManager:60 - Deleting directory /tmp/spark-2e51f3c3-bc71-4a50-9195-f6b36579d848
2024-09-18 05:59:41 [main] INFO  HiveConf:187 - Found configuration file file:/home/hive/apache-hive-3.1.3-bin/conf/hive-site.xml
2024-09-18 05:59:41 [main] INFO  SparkContext:60 - Running Spark version 3.5.2
2024-09-18 05:59:41 [main] INFO  SparkContext:60 - OS info Linux, 5.15.49-linuxkit, aarch64
2024-09-18 05:59:41 [main] INFO  SparkContext:60 - Java version 1.8.0_362
2024-09-18 05:59:41 [main] WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2024-09-18 05:59:41 [main] INFO  ResourceUtils:60 - ==============================================================
2024-09-18 05:59:41 [main] INFO  ResourceUtils:60 - No custom resources configured for spark.driver.
2024-09-18 05:59:41 [main] INFO  ResourceUtils:60 - ==============================================================
2024-09-18 05:59:41 [main] INFO  SparkContext:60 - Submitted application: CSV to Hive
2024-09-18 05:59:41 [main] INFO  ResourceProfile:60 - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2024-09-18 05:59:41 [main] INFO  ResourceProfile:60 - Limiting resource is cpu
2024-09-18 05:59:41 [main] INFO  ResourceProfileManager:60 - Added ResourceProfile id: 0
2024-09-18 05:59:41 [main] INFO  SecurityManager:60 - Changing view acls to: root
2024-09-18 05:59:41 [main] INFO  SecurityManager:60 - Changing modify acls to: root
2024-09-18 05:59:41 [main] INFO  SecurityManager:60 - Changing view acls groups to: 
2024-09-18 05:59:41 [main] INFO  SecurityManager:60 - Changing modify acls groups to: 
2024-09-18 05:59:41 [main] INFO  SecurityManager:60 - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
2024-09-18 05:59:41 [main] INFO  Utils:60 - Successfully started service 'sparkDriver' on port 35105.
2024-09-18 05:59:42 [main] INFO  SparkEnv:60 - Registering MapOutputTracker
2024-09-18 05:59:42 [main] INFO  SparkEnv:60 - Registering BlockManagerMaster
2024-09-18 05:59:42 [main] INFO  BlockManagerMasterEndpoint:60 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2024-09-18 05:59:42 [main] INFO  BlockManagerMasterEndpoint:60 - BlockManagerMasterEndpoint up
2024-09-18 05:59:42 [main] INFO  SparkEnv:60 - Registering BlockManagerMasterHeartbeat
2024-09-18 05:59:42 [main] INFO  DiskBlockManager:60 - Created local directory at /tmp/blockmgr-aba74975-0083-4dcb-b310-c7e238e87d09
2024-09-18 05:59:42 [main] INFO  MemoryStore:60 - MemoryStore started with capacity 1147.2 MiB
2024-09-18 05:59:42 [main] INFO  SparkEnv:60 - Registering OutputCommitCoordinator
2024-09-18 05:59:42 [main] INFO  log:170 - Logging initialized @1636ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-09-18 05:59:42 [main] INFO  JettyUtils:60 - Start Jetty 0.0.0.0:4040 for SparkUI
2024-09-18 05:59:42 [main] INFO  Server:375 - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 1.8.0_362-8u372-ga~us1-0ubuntu1~22.04-b09
2024-09-18 05:59:42 [main] INFO  Server:415 - Started @1724ms
2024-09-18 05:59:42 [main] INFO  AbstractConnector:333 - Started ServerConnector@51827393{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-18 05:59:42 [main] INFO  Utils:60 - Successfully started service 'SparkUI' on port 4040.
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@213860b8{/,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  Executor:60 - Starting executor ID driver on host nn
2024-09-18 05:59:42 [main] INFO  Executor:60 - OS info Linux, 5.15.49-linuxkit, aarch64
2024-09-18 05:59:42 [main] INFO  Executor:60 - Java version 1.8.0_362
2024-09-18 05:59:42 [main] INFO  Executor:60 - Starting executor with user classpath (userClassPathFirst = false): ''
2024-09-18 05:59:42 [main] INFO  Executor:60 - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@655523dd for default.
2024-09-18 05:59:42 [main] INFO  Utils:60 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44681.
2024-09-18 05:59:42 [main] INFO  NettyBlockTransferService:84 - Server created on nn:44681
2024-09-18 05:59:42 [main] INFO  BlockManager:60 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2024-09-18 05:59:42 [main] INFO  BlockManagerMaster:60 - Registering BlockManager BlockManagerId(driver, nn, 44681, None)
2024-09-18 05:59:42 [dispatcher-BlockManagerMaster] INFO  BlockManagerMasterEndpoint:60 - Registering block manager nn:44681 with 1147.2 MiB RAM, BlockManagerId(driver, nn, 44681, None)
2024-09-18 05:59:42 [main] INFO  BlockManagerMaster:60 - Registered BlockManager BlockManagerId(driver, nn, 44681, None)
2024-09-18 05:59:42 [main] INFO  BlockManager:60 - Initialized BlockManager: BlockManagerId(driver, nn, 44681, None)
2024-09-18 05:59:42 [main] INFO  ContextHandler:1159 - Stopped o.s.j.s.ServletContextHandler@213860b8{/,null,STOPPED,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4e17913b{/jobs,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@64f16277{/jobs/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4e6f2bb5{/jobs/job,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3f628ce9{/jobs/job/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@26d96e5{/stages,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1846579f{/stages/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@5fac521d{/stages/stage,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@129bd55d{/stages/stage/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3abfe845{/stages/pool,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3672276e{/stages/pool/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7f08caf{/storage,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2330e3e0{/storage/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@27a2a089{/storage/rdd,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@706eab5d{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@40e60ece{/environment,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3a230001{/environment/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2aa6311a{/executors,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@249e0271{/executors/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@53a665ad{/executors/threadDump,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@78525ef9{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4d654825{/static,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4339e0de{/,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@61d84e08{/api,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@107f4980{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1d540566{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3a01773b{/metrics/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  SharedState:60 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2024-09-18 05:59:42 [main] INFO  SharedState:60 - Warehouse path is 'hdfs://nn:9000/user/hive/warehouse'.
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@32e652b6{/SQL,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7593ea79{/SQL/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@68ee3b6d{/SQL/execution,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2be95d31{/SQL/execution/json,null,AVAILABLE,@Spark}
2024-09-18 05:59:42 [main] INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@a5bbaec{/static/sql,null,AVAILABLE,@Spark}
2024-09-18 05:59:43 [main] INFO  InMemoryFileIndex:60 - It took 14 ms to list leaf files for 1 paths.
2024-09-18 05:59:43 [main] INFO  InMemoryFileIndex:60 - It took 0 ms to list leaf files for 1 paths.
2024-09-18 05:59:44 [main] INFO  FileSourceStrategy:60 - Pushed Filters: 
2024-09-18 05:59:44 [main] INFO  FileSourceStrategy:60 - Post-Scan Filters: (length(trim(value#0, None)) > 0)
2024-09-18 05:59:45 [main] INFO  CodeGenerator:60 - Code generated in 102.172625 ms
2024-09-18 05:59:45 [main] INFO  MemoryStore:60 - Block broadcast_0 stored as values in memory (estimated size 352.4 KiB, free 1146.9 MiB)
2024-09-18 05:59:45 [main] INFO  MemoryStore:60 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 1146.8 MiB)
2024-09-18 05:59:45 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_0_piece0 in memory on nn:44681 (size: 34.2 KiB, free: 1147.2 MiB)
2024-09-18 05:59:45 [main] INFO  SparkContext:60 - Created broadcast 0 from load at UserActivityProcessor.java:28
2024-09-18 05:59:45 [main] INFO  FileSourceScanExec:60 - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2024-09-18 05:59:45 [main] INFO  SparkContext:60 - Starting job: load at UserActivityProcessor.java:28
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Got job 0 (load at UserActivityProcessor.java:28) with 1 output partitions
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Final stage: ResultStage 0 (load at UserActivityProcessor.java:28)
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Parents of final stage: List()
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Missing parents: List()
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting ResultStage 0 (MapPartitionsRDD[3] at load at UserActivityProcessor.java:28), which has no missing parents
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 1146.8 MiB)
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 1146.8 MiB)
2024-09-18 05:59:45 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_1_piece0 in memory on nn:44681 (size: 6.4 KiB, free: 1147.2 MiB)
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  SparkContext:60 - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at UserActivityProcessor.java:28) (first 15 tasks are for partitions Vector(0))
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Adding task set 0.0 with 1 tasks resource profile 0
2024-09-18 05:59:45 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 0.0 in stage 0.0 (TID 0) (nn, executor driver, partition 0, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  Executor:60 - Running task 0.0 in stage 0.0 (TID 0)
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  CodeGenerator:60 - Code generated in 7.374417 ms
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 0-134217728, partition values: [empty row]
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  CodeGenerator:60 - Code generated in 6.596958 ms
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  Executor:60 - Finished task 0.0 in stage 0.0 (TID 0). 1694 bytes result sent to driver
2024-09-18 05:59:45 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 0.0 in stage 0.0 (TID 0) in 274 ms on nn (executor driver) (1/1)
2024-09-18 05:59:45 [task-result-getter-0] INFO  TaskSchedulerImpl:60 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - ResultStage 0 (load at UserActivityProcessor.java:28) finished in 0.343 s
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Killing all running tasks in stage 0: Stage finished
2024-09-18 05:59:45 [main] INFO  DAGScheduler:60 - Job 0 finished: load at UserActivityProcessor.java:28, took 0.368400 s
2024-09-18 05:59:45 [main] INFO  CodeGenerator:60 - Code generated in 6.767875 ms
2024-09-18 05:59:45 [main] INFO  FileSourceStrategy:60 - Pushed Filters: 
2024-09-18 05:59:45 [main] INFO  FileSourceStrategy:60 - Post-Scan Filters: 
2024-09-18 05:59:45 [main] INFO  MemoryStore:60 - Block broadcast_2 stored as values in memory (estimated size 352.4 KiB, free 1146.5 MiB)
2024-09-18 05:59:45 [main] INFO  MemoryStore:60 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 1146.4 MiB)
2024-09-18 05:59:45 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_2_piece0 in memory on nn:44681 (size: 34.2 KiB, free: 1147.1 MiB)
2024-09-18 05:59:45 [main] INFO  SparkContext:60 - Created broadcast 2 from load at UserActivityProcessor.java:28
2024-09-18 05:59:45 [main] INFO  FileSourceScanExec:60 - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2024-09-18 05:59:45 [main] INFO  SparkContext:60 - Starting job: load at UserActivityProcessor.java:28
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Got job 1 (load at UserActivityProcessor.java:28) with 43 output partitions
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Final stage: ResultStage 1 (load at UserActivityProcessor.java:28)
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Parents of final stage: List()
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Missing parents: List()
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting ResultStage 1 (MapPartitionsRDD[9] at load at UserActivityProcessor.java:28), which has no missing parents
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_3 stored as values in memory (estimated size 27.7 KiB, free 1146.4 MiB)
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 1146.4 MiB)
2024-09-18 05:59:45 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_3_piece0 in memory on nn:44681 (size: 12.7 KiB, free: 1147.1 MiB)
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  SparkContext:60 - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting 43 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at UserActivityProcessor.java:28) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2024-09-18 05:59:45 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Adding task set 1.0 with 43 tasks resource profile 0
2024-09-18 05:59:45 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 0.0 in stage 1.0 (TID 1) (nn, executor driver, partition 0, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:45 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 1.0 in stage 1.0 (TID 2) (nn, executor driver, partition 1, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:45 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 2.0 in stage 1.0 (TID 3) (nn, executor driver, partition 2, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:45 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 3.0 in stage 1.0 (TID 4) (nn, executor driver, partition 3, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  Executor:60 - Running task 0.0 in stage 1.0 (TID 1)
2024-09-18 05:59:45 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  Executor:60 - Running task 1.0 in stage 1.0 (TID 2)
2024-09-18 05:59:45 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  Executor:60 - Running task 3.0 in stage 1.0 (TID 4)
2024-09-18 05:59:45 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  Executor:60 - Running task 2.0 in stage 1.0 (TID 3)
2024-09-18 05:59:45 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Removed broadcast_1_piece0 on nn:44681 in memory (size: 6.4 KiB, free: 1147.1 MiB)
2024-09-18 05:59:45 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  CodeGenerator:60 - Code generated in 3.791208 ms
2024-09-18 05:59:45 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 268435456-402653184, partition values: [empty row]
2024-09-18 05:59:45 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 0-134217728, partition values: [empty row]
2024-09-18 05:59:45 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 402653184-536870912, partition values: [empty row]
2024-09-18 05:59:45 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 134217728-268435456, partition values: [empty row]
2024-09-18 05:59:49 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  Executor:60 - Finished task 0.0 in stage 1.0 (TID 1). 1843 bytes result sent to driver
2024-09-18 05:59:49 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 4.0 in stage 1.0 (TID 5) (nn, executor driver, partition 4, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:49 [Executor task launch worker for task 4.0 in stage 1.0 (TID 5)] INFO  Executor:60 - Running task 4.0 in stage 1.0 (TID 5)
2024-09-18 05:59:49 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 0.0 in stage 1.0 (TID 1) in 3474 ms on nn (executor driver) (1/43)
2024-09-18 05:59:49 [Executor task launch worker for task 4.0 in stage 1.0 (TID 5)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 536870912-671088640, partition values: [empty row]
2024-09-18 05:59:49 [Executor task launch worker for task 3.0 in stage 1.0 (TID 4)] INFO  Executor:60 - Finished task 3.0 in stage 1.0 (TID 4). 1800 bytes result sent to driver
2024-09-18 05:59:49 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 5.0 in stage 1.0 (TID 6) (nn, executor driver, partition 5, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:49 [Executor task launch worker for task 5.0 in stage 1.0 (TID 6)] INFO  Executor:60 - Running task 5.0 in stage 1.0 (TID 6)
2024-09-18 05:59:49 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 3.0 in stage 1.0 (TID 4) in 3718 ms on nn (executor driver) (2/43)
2024-09-18 05:59:49 [Executor task launch worker for task 5.0 in stage 1.0 (TID 6)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 671088640-805306368, partition values: [empty row]
2024-09-18 05:59:49 [Executor task launch worker for task 2.0 in stage 1.0 (TID 3)] INFO  Executor:60 - Finished task 2.0 in stage 1.0 (TID 3). 1800 bytes result sent to driver
2024-09-18 05:59:49 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 6.0 in stage 1.0 (TID 7) (nn, executor driver, partition 6, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:49 [Executor task launch worker for task 6.0 in stage 1.0 (TID 7)] INFO  Executor:60 - Running task 6.0 in stage 1.0 (TID 7)
2024-09-18 05:59:49 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 2.0 in stage 1.0 (TID 3) in 3819 ms on nn (executor driver) (3/43)
2024-09-18 05:59:49 [Executor task launch worker for task 6.0 in stage 1.0 (TID 7)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 805306368-939524096, partition values: [empty row]
2024-09-18 05:59:49 [Executor task launch worker for task 1.0 in stage 1.0 (TID 2)] INFO  Executor:60 - Finished task 1.0 in stage 1.0 (TID 2). 1800 bytes result sent to driver
2024-09-18 05:59:49 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 7.0 in stage 1.0 (TID 8) (nn, executor driver, partition 7, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:49 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 1.0 in stage 1.0 (TID 2) in 4012 ms on nn (executor driver) (4/43)
2024-09-18 05:59:49 [Executor task launch worker for task 7.0 in stage 1.0 (TID 8)] INFO  Executor:60 - Running task 7.0 in stage 1.0 (TID 8)
2024-09-18 05:59:49 [Executor task launch worker for task 7.0 in stage 1.0 (TID 8)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 939524096-1073741824, partition values: [empty row]
2024-09-18 05:59:52 [Executor task launch worker for task 4.0 in stage 1.0 (TID 5)] INFO  Executor:60 - Finished task 4.0 in stage 1.0 (TID 5). 1800 bytes result sent to driver
2024-09-18 05:59:52 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 8.0 in stage 1.0 (TID 9) (nn, executor driver, partition 8, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:52 [Executor task launch worker for task 8.0 in stage 1.0 (TID 9)] INFO  Executor:60 - Running task 8.0 in stage 1.0 (TID 9)
2024-09-18 05:59:52 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 4.0 in stage 1.0 (TID 5) in 3489 ms on nn (executor driver) (5/43)
2024-09-18 05:59:52 [Executor task launch worker for task 8.0 in stage 1.0 (TID 9)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1073741824-1207959552, partition values: [empty row]
2024-09-18 05:59:52 [Executor task launch worker for task 5.0 in stage 1.0 (TID 6)] INFO  Executor:60 - Finished task 5.0 in stage 1.0 (TID 6). 1800 bytes result sent to driver
2024-09-18 05:59:52 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 9.0 in stage 1.0 (TID 10) (nn, executor driver, partition 9, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:52 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 5.0 in stage 1.0 (TID 6) in 3365 ms on nn (executor driver) (6/43)
2024-09-18 05:59:52 [Executor task launch worker for task 9.0 in stage 1.0 (TID 10)] INFO  Executor:60 - Running task 9.0 in stage 1.0 (TID 10)
2024-09-18 05:59:52 [Executor task launch worker for task 9.0 in stage 1.0 (TID 10)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1207959552-1342177280, partition values: [empty row]
2024-09-18 05:59:53 [Executor task launch worker for task 6.0 in stage 1.0 (TID 7)] INFO  Executor:60 - Finished task 6.0 in stage 1.0 (TID 7). 1800 bytes result sent to driver
2024-09-18 05:59:53 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 10.0 in stage 1.0 (TID 11) (nn, executor driver, partition 10, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:53 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 6.0 in stage 1.0 (TID 7) in 3357 ms on nn (executor driver) (7/43)
2024-09-18 05:59:53 [Executor task launch worker for task 10.0 in stage 1.0 (TID 11)] INFO  Executor:60 - Running task 10.0 in stage 1.0 (TID 11)
2024-09-18 05:59:53 [Executor task launch worker for task 10.0 in stage 1.0 (TID 11)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1342177280-1476395008, partition values: [empty row]
2024-09-18 05:59:53 [Executor task launch worker for task 7.0 in stage 1.0 (TID 8)] INFO  Executor:60 - Finished task 7.0 in stage 1.0 (TID 8). 1800 bytes result sent to driver
2024-09-18 05:59:53 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 11.0 in stage 1.0 (TID 12) (nn, executor driver, partition 11, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:53 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 7.0 in stage 1.0 (TID 8) in 3476 ms on nn (executor driver) (8/43)
2024-09-18 05:59:53 [Executor task launch worker for task 11.0 in stage 1.0 (TID 12)] INFO  Executor:60 - Running task 11.0 in stage 1.0 (TID 12)
2024-09-18 05:59:53 [Executor task launch worker for task 11.0 in stage 1.0 (TID 12)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1476395008-1610612736, partition values: [empty row]
2024-09-18 05:59:56 [Executor task launch worker for task 9.0 in stage 1.0 (TID 10)] INFO  Executor:60 - Finished task 9.0 in stage 1.0 (TID 10). 1800 bytes result sent to driver
2024-09-18 05:59:56 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 12.0 in stage 1.0 (TID 13) (nn, executor driver, partition 12, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:56 [Executor task launch worker for task 12.0 in stage 1.0 (TID 13)] INFO  Executor:60 - Running task 12.0 in stage 1.0 (TID 13)
2024-09-18 05:59:56 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 9.0 in stage 1.0 (TID 10) in 3085 ms on nn (executor driver) (9/43)
2024-09-18 05:59:56 [Executor task launch worker for task 12.0 in stage 1.0 (TID 13)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1610612736-1744830464, partition values: [empty row]
2024-09-18 05:59:56 [Executor task launch worker for task 10.0 in stage 1.0 (TID 11)] INFO  Executor:60 - Finished task 10.0 in stage 1.0 (TID 11). 1800 bytes result sent to driver
2024-09-18 05:59:56 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 13.0 in stage 1.0 (TID 14) (nn, executor driver, partition 13, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:56 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 10.0 in stage 1.0 (TID 11) in 3166 ms on nn (executor driver) (10/43)
2024-09-18 05:59:56 [Executor task launch worker for task 13.0 in stage 1.0 (TID 14)] INFO  Executor:60 - Running task 13.0 in stage 1.0 (TID 14)
2024-09-18 05:59:56 [Executor task launch worker for task 13.0 in stage 1.0 (TID 14)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1744830464-1879048192, partition values: [empty row]
2024-09-18 05:59:56 [Executor task launch worker for task 8.0 in stage 1.0 (TID 9)] INFO  Executor:60 - Finished task 8.0 in stage 1.0 (TID 9). 1800 bytes result sent to driver
2024-09-18 05:59:56 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 14.0 in stage 1.0 (TID 15) (nn, executor driver, partition 14, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:56 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 8.0 in stage 1.0 (TID 9) in 3445 ms on nn (executor driver) (11/43)
2024-09-18 05:59:56 [Executor task launch worker for task 14.0 in stage 1.0 (TID 15)] INFO  Executor:60 - Running task 14.0 in stage 1.0 (TID 15)
2024-09-18 05:59:56 [Executor task launch worker for task 14.0 in stage 1.0 (TID 15)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1879048192-2013265920, partition values: [empty row]
2024-09-18 05:59:56 [Executor task launch worker for task 11.0 in stage 1.0 (TID 12)] INFO  Executor:60 - Finished task 11.0 in stage 1.0 (TID 12). 1800 bytes result sent to driver
2024-09-18 05:59:56 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 15.0 in stage 1.0 (TID 16) (nn, executor driver, partition 15, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:56 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 11.0 in stage 1.0 (TID 12) in 2988 ms on nn (executor driver) (12/43)
2024-09-18 05:59:56 [Executor task launch worker for task 15.0 in stage 1.0 (TID 16)] INFO  Executor:60 - Running task 15.0 in stage 1.0 (TID 16)
2024-09-18 05:59:56 [Executor task launch worker for task 15.0 in stage 1.0 (TID 16)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2013265920-2147483648, partition values: [empty row]
2024-09-18 05:59:59 [Executor task launch worker for task 12.0 in stage 1.0 (TID 13)] INFO  Executor:60 - Finished task 12.0 in stage 1.0 (TID 13). 1800 bytes result sent to driver
2024-09-18 05:59:59 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 16.0 in stage 1.0 (TID 17) (nn, executor driver, partition 16, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:59 [Executor task launch worker for task 16.0 in stage 1.0 (TID 17)] INFO  Executor:60 - Running task 16.0 in stage 1.0 (TID 17)
2024-09-18 05:59:59 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 12.0 in stage 1.0 (TID 13) in 3087 ms on nn (executor driver) (13/43)
2024-09-18 05:59:59 [Executor task launch worker for task 16.0 in stage 1.0 (TID 17)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2147483648-2281701376, partition values: [empty row]
2024-09-18 05:59:59 [Executor task launch worker for task 14.0 in stage 1.0 (TID 15)] INFO  Executor:60 - Finished task 14.0 in stage 1.0 (TID 15). 1800 bytes result sent to driver
2024-09-18 05:59:59 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 17.0 in stage 1.0 (TID 18) (nn, executor driver, partition 17, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:59 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 14.0 in stage 1.0 (TID 15) in 2984 ms on nn (executor driver) (14/43)
2024-09-18 05:59:59 [Executor task launch worker for task 17.0 in stage 1.0 (TID 18)] INFO  Executor:60 - Running task 17.0 in stage 1.0 (TID 18)
2024-09-18 05:59:59 [Executor task launch worker for task 17.0 in stage 1.0 (TID 18)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2281701376-2415919104, partition values: [empty row]
2024-09-18 05:59:59 [Executor task launch worker for task 13.0 in stage 1.0 (TID 14)] INFO  Executor:60 - Finished task 13.0 in stage 1.0 (TID 14). 1800 bytes result sent to driver
2024-09-18 05:59:59 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 18.0 in stage 1.0 (TID 19) (nn, executor driver, partition 18, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:59 [Executor task launch worker for task 18.0 in stage 1.0 (TID 19)] INFO  Executor:60 - Running task 18.0 in stage 1.0 (TID 19)
2024-09-18 05:59:59 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 13.0 in stage 1.0 (TID 14) in 3116 ms on nn (executor driver) (15/43)
2024-09-18 05:59:59 [Executor task launch worker for task 18.0 in stage 1.0 (TID 19)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2415919104-2550136832, partition values: [empty row]
2024-09-18 05:59:59 [Executor task launch worker for task 15.0 in stage 1.0 (TID 16)] INFO  Executor:60 - Finished task 15.0 in stage 1.0 (TID 16). 1800 bytes result sent to driver
2024-09-18 05:59:59 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 19.0 in stage 1.0 (TID 20) (nn, executor driver, partition 19, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 05:59:59 [Executor task launch worker for task 19.0 in stage 1.0 (TID 20)] INFO  Executor:60 - Running task 19.0 in stage 1.0 (TID 20)
2024-09-18 05:59:59 [Executor task launch worker for task 19.0 in stage 1.0 (TID 20)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2550136832-2684354560, partition values: [empty row]
2024-09-18 05:59:59 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 15.0 in stage 1.0 (TID 16) in 3051 ms on nn (executor driver) (16/43)
2024-09-18 06:00:02 [Executor task launch worker for task 16.0 in stage 1.0 (TID 17)] INFO  Executor:60 - Finished task 16.0 in stage 1.0 (TID 17). 1843 bytes result sent to driver
2024-09-18 06:00:02 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 20.0 in stage 1.0 (TID 21) (nn, executor driver, partition 20, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:02 [Executor task launch worker for task 20.0 in stage 1.0 (TID 21)] INFO  Executor:60 - Running task 20.0 in stage 1.0 (TID 21)
2024-09-18 06:00:02 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 16.0 in stage 1.0 (TID 17) in 2902 ms on nn (executor driver) (17/43)
2024-09-18 06:00:02 [Executor task launch worker for task 20.0 in stage 1.0 (TID 21)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2684354560-2818572288, partition values: [empty row]
2024-09-18 06:00:02 [Executor task launch worker for task 18.0 in stage 1.0 (TID 19)] INFO  Executor:60 - Finished task 18.0 in stage 1.0 (TID 19). 1800 bytes result sent to driver
2024-09-18 06:00:02 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 21.0 in stage 1.0 (TID 22) (nn, executor driver, partition 21, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:02 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 18.0 in stage 1.0 (TID 19) in 2970 ms on nn (executor driver) (18/43)
2024-09-18 06:00:02 [Executor task launch worker for task 21.0 in stage 1.0 (TID 22)] INFO  Executor:60 - Running task 21.0 in stage 1.0 (TID 22)
2024-09-18 06:00:02 [Executor task launch worker for task 21.0 in stage 1.0 (TID 22)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2818572288-2952790016, partition values: [empty row]
2024-09-18 06:00:02 [Executor task launch worker for task 19.0 in stage 1.0 (TID 20)] INFO  Executor:60 - Finished task 19.0 in stage 1.0 (TID 20). 1800 bytes result sent to driver
2024-09-18 06:00:02 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 22.0 in stage 1.0 (TID 23) (nn, executor driver, partition 22, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:02 [Executor task launch worker for task 22.0 in stage 1.0 (TID 23)] INFO  Executor:60 - Running task 22.0 in stage 1.0 (TID 23)
2024-09-18 06:00:02 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 19.0 in stage 1.0 (TID 20) in 2939 ms on nn (executor driver) (19/43)
2024-09-18 06:00:02 [Executor task launch worker for task 22.0 in stage 1.0 (TID 23)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2952790016-3087007744, partition values: [empty row]
2024-09-18 06:00:02 [Executor task launch worker for task 17.0 in stage 1.0 (TID 18)] INFO  Executor:60 - Finished task 17.0 in stage 1.0 (TID 18). 1800 bytes result sent to driver
2024-09-18 06:00:02 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 23.0 in stage 1.0 (TID 24) (nn, executor driver, partition 23, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:02 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 17.0 in stage 1.0 (TID 18) in 3212 ms on nn (executor driver) (20/43)
2024-09-18 06:00:02 [Executor task launch worker for task 23.0 in stage 1.0 (TID 24)] INFO  Executor:60 - Running task 23.0 in stage 1.0 (TID 24)
2024-09-18 06:00:02 [Executor task launch worker for task 23.0 in stage 1.0 (TID 24)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3087007744-3221225472, partition values: [empty row]
2024-09-18 06:00:04 [Executor task launch worker for task 20.0 in stage 1.0 (TID 21)] INFO  Executor:60 - Finished task 20.0 in stage 1.0 (TID 21). 1800 bytes result sent to driver
2024-09-18 06:00:04 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 24.0 in stage 1.0 (TID 25) (nn, executor driver, partition 24, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:04 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 20.0 in stage 1.0 (TID 21) in 2564 ms on nn (executor driver) (21/43)
2024-09-18 06:00:04 [Executor task launch worker for task 24.0 in stage 1.0 (TID 25)] INFO  Executor:60 - Running task 24.0 in stage 1.0 (TID 25)
2024-09-18 06:00:04 [Executor task launch worker for task 24.0 in stage 1.0 (TID 25)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3221225472-3355443200, partition values: [empty row]
2024-09-18 06:00:04 [Executor task launch worker for task 23.0 in stage 1.0 (TID 24)] INFO  Executor:60 - Finished task 23.0 in stage 1.0 (TID 24). 1800 bytes result sent to driver
2024-09-18 06:00:04 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 25.0 in stage 1.0 (TID 26) (nn, executor driver, partition 25, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:04 [Executor task launch worker for task 25.0 in stage 1.0 (TID 26)] INFO  Executor:60 - Running task 25.0 in stage 1.0 (TID 26)
2024-09-18 06:00:04 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 23.0 in stage 1.0 (TID 24) in 2474 ms on nn (executor driver) (22/43)
2024-09-18 06:00:04 [Executor task launch worker for task 25.0 in stage 1.0 (TID 26)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3355443200-3489660928, partition values: [empty row]
2024-09-18 06:00:04 [Executor task launch worker for task 21.0 in stage 1.0 (TID 22)] INFO  Executor:60 - Finished task 21.0 in stage 1.0 (TID 22). 1800 bytes result sent to driver
2024-09-18 06:00:04 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 26.0 in stage 1.0 (TID 27) (nn, executor driver, partition 26, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:04 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 21.0 in stage 1.0 (TID 22) in 2692 ms on nn (executor driver) (23/43)
2024-09-18 06:00:04 [Executor task launch worker for task 26.0 in stage 1.0 (TID 27)] INFO  Executor:60 - Running task 26.0 in stage 1.0 (TID 27)
2024-09-18 06:00:04 [Executor task launch worker for task 26.0 in stage 1.0 (TID 27)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3489660928-3623878656, partition values: [empty row]
2024-09-18 06:00:05 [Executor task launch worker for task 22.0 in stage 1.0 (TID 23)] INFO  Executor:60 - Finished task 22.0 in stage 1.0 (TID 23). 1800 bytes result sent to driver
2024-09-18 06:00:05 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 27.0 in stage 1.0 (TID 28) (nn, executor driver, partition 27, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:05 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 22.0 in stage 1.0 (TID 23) in 2872 ms on nn (executor driver) (24/43)
2024-09-18 06:00:05 [Executor task launch worker for task 27.0 in stage 1.0 (TID 28)] INFO  Executor:60 - Running task 27.0 in stage 1.0 (TID 28)
2024-09-18 06:00:05 [Executor task launch worker for task 27.0 in stage 1.0 (TID 28)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3623878656-3758096384, partition values: [empty row]
2024-09-18 06:00:07 [Executor task launch worker for task 24.0 in stage 1.0 (TID 25)] INFO  Executor:60 - Finished task 24.0 in stage 1.0 (TID 25). 1800 bytes result sent to driver
2024-09-18 06:00:07 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 28.0 in stage 1.0 (TID 29) (nn, executor driver, partition 28, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:07 [Executor task launch worker for task 28.0 in stage 1.0 (TID 29)] INFO  Executor:60 - Running task 28.0 in stage 1.0 (TID 29)
2024-09-18 06:00:07 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 24.0 in stage 1.0 (TID 25) in 2603 ms on nn (executor driver) (25/43)
2024-09-18 06:00:07 [Executor task launch worker for task 28.0 in stage 1.0 (TID 29)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3758096384-3892314112, partition values: [empty row]
2024-09-18 06:00:07 [Executor task launch worker for task 25.0 in stage 1.0 (TID 26)] INFO  Executor:60 - Finished task 25.0 in stage 1.0 (TID 26). 1800 bytes result sent to driver
2024-09-18 06:00:07 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 29.0 in stage 1.0 (TID 30) (nn, executor driver, partition 29, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:07 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 25.0 in stage 1.0 (TID 26) in 2826 ms on nn (executor driver) (26/43)
2024-09-18 06:00:07 [Executor task launch worker for task 29.0 in stage 1.0 (TID 30)] INFO  Executor:60 - Running task 29.0 in stage 1.0 (TID 30)
2024-09-18 06:00:07 [Executor task launch worker for task 29.0 in stage 1.0 (TID 30)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3892314112-4026531840, partition values: [empty row]
2024-09-18 06:00:07 [Executor task launch worker for task 27.0 in stage 1.0 (TID 28)] INFO  Executor:60 - Finished task 27.0 in stage 1.0 (TID 28). 1800 bytes result sent to driver
2024-09-18 06:00:07 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 30.0 in stage 1.0 (TID 31) (nn, executor driver, partition 30, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:07 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 27.0 in stage 1.0 (TID 28) in 2603 ms on nn (executor driver) (27/43)
2024-09-18 06:00:07 [Executor task launch worker for task 30.0 in stage 1.0 (TID 31)] INFO  Executor:60 - Running task 30.0 in stage 1.0 (TID 31)
2024-09-18 06:00:07 [Executor task launch worker for task 30.0 in stage 1.0 (TID 31)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4026531840-4160749568, partition values: [empty row]
2024-09-18 06:00:07 [Executor task launch worker for task 26.0 in stage 1.0 (TID 27)] INFO  Executor:60 - Finished task 26.0 in stage 1.0 (TID 27). 1800 bytes result sent to driver
2024-09-18 06:00:07 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 31.0 in stage 1.0 (TID 32) (nn, executor driver, partition 31, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:07 [Executor task launch worker for task 31.0 in stage 1.0 (TID 32)] INFO  Executor:60 - Running task 31.0 in stage 1.0 (TID 32)
2024-09-18 06:00:07 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 26.0 in stage 1.0 (TID 27) in 2932 ms on nn (executor driver) (28/43)
2024-09-18 06:00:07 [Executor task launch worker for task 31.0 in stage 1.0 (TID 32)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4160749568-4294967296, partition values: [empty row]
2024-09-18 06:00:09 [Executor task launch worker for task 28.0 in stage 1.0 (TID 29)] INFO  Executor:60 - Finished task 28.0 in stage 1.0 (TID 29). 1843 bytes result sent to driver
2024-09-18 06:00:09 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 32.0 in stage 1.0 (TID 33) (nn, executor driver, partition 32, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:09 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 28.0 in stage 1.0 (TID 29) in 2760 ms on nn (executor driver) (29/43)
2024-09-18 06:00:09 [Executor task launch worker for task 32.0 in stage 1.0 (TID 33)] INFO  Executor:60 - Running task 32.0 in stage 1.0 (TID 33)
2024-09-18 06:00:09 [Executor task launch worker for task 32.0 in stage 1.0 (TID 33)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4294967296-4429185024, partition values: [empty row]
2024-09-18 06:00:10 [Executor task launch worker for task 29.0 in stage 1.0 (TID 30)] INFO  Executor:60 - Finished task 29.0 in stage 1.0 (TID 30). 1800 bytes result sent to driver
2024-09-18 06:00:10 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 33.0 in stage 1.0 (TID 34) (nn, executor driver, partition 33, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:10 [Executor task launch worker for task 33.0 in stage 1.0 (TID 34)] INFO  Executor:60 - Running task 33.0 in stage 1.0 (TID 34)
2024-09-18 06:00:10 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 29.0 in stage 1.0 (TID 30) in 3051 ms on nn (executor driver) (30/43)
2024-09-18 06:00:10 [Executor task launch worker for task 33.0 in stage 1.0 (TID 34)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4429185024-4563402752, partition values: [empty row]
2024-09-18 06:00:10 [Executor task launch worker for task 30.0 in stage 1.0 (TID 31)] INFO  Executor:60 - Finished task 30.0 in stage 1.0 (TID 31). 1800 bytes result sent to driver
2024-09-18 06:00:10 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 34.0 in stage 1.0 (TID 35) (nn, executor driver, partition 34, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:10 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 30.0 in stage 1.0 (TID 31) in 3132 ms on nn (executor driver) (31/43)
2024-09-18 06:00:10 [Executor task launch worker for task 34.0 in stage 1.0 (TID 35)] INFO  Executor:60 - Running task 34.0 in stage 1.0 (TID 35)
2024-09-18 06:00:10 [Executor task launch worker for task 34.0 in stage 1.0 (TID 35)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4563402752-4697620480, partition values: [empty row]
2024-09-18 06:00:11 [Executor task launch worker for task 31.0 in stage 1.0 (TID 32)] INFO  Executor:60 - Finished task 31.0 in stage 1.0 (TID 32). 1800 bytes result sent to driver
2024-09-18 06:00:11 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 35.0 in stage 1.0 (TID 36) (nn, executor driver, partition 35, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:11 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 31.0 in stage 1.0 (TID 32) in 3199 ms on nn (executor driver) (32/43)
2024-09-18 06:00:11 [Executor task launch worker for task 35.0 in stage 1.0 (TID 36)] INFO  Executor:60 - Running task 35.0 in stage 1.0 (TID 36)
2024-09-18 06:00:11 [Executor task launch worker for task 35.0 in stage 1.0 (TID 36)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4697620480-4831838208, partition values: [empty row]
2024-09-18 06:00:13 [Executor task launch worker for task 32.0 in stage 1.0 (TID 33)] INFO  Executor:60 - Finished task 32.0 in stage 1.0 (TID 33). 1800 bytes result sent to driver
2024-09-18 06:00:13 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 36.0 in stage 1.0 (TID 37) (nn, executor driver, partition 36, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:13 [Executor task launch worker for task 36.0 in stage 1.0 (TID 37)] INFO  Executor:60 - Running task 36.0 in stage 1.0 (TID 37)
2024-09-18 06:00:13 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 32.0 in stage 1.0 (TID 33) in 3245 ms on nn (executor driver) (33/43)
2024-09-18 06:00:13 [Executor task launch worker for task 36.0 in stage 1.0 (TID 37)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4831838208-4966055936, partition values: [empty row]
2024-09-18 06:00:13 [Executor task launch worker for task 33.0 in stage 1.0 (TID 34)] INFO  Executor:60 - Finished task 33.0 in stage 1.0 (TID 34). 1800 bytes result sent to driver
2024-09-18 06:00:13 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 37.0 in stage 1.0 (TID 38) (nn, executor driver, partition 37, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:13 [Executor task launch worker for task 37.0 in stage 1.0 (TID 38)] INFO  Executor:60 - Running task 37.0 in stage 1.0 (TID 38)
2024-09-18 06:00:13 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 33.0 in stage 1.0 (TID 34) in 2972 ms on nn (executor driver) (34/43)
2024-09-18 06:00:13 [Executor task launch worker for task 37.0 in stage 1.0 (TID 38)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4966055936-5100273664, partition values: [empty row]
2024-09-18 06:00:14 [Executor task launch worker for task 34.0 in stage 1.0 (TID 35)] INFO  Executor:60 - Finished task 34.0 in stage 1.0 (TID 35). 1800 bytes result sent to driver
2024-09-18 06:00:14 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 38.0 in stage 1.0 (TID 39) (nn, executor driver, partition 38, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:14 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 34.0 in stage 1.0 (TID 35) in 3114 ms on nn (executor driver) (35/43)
2024-09-18 06:00:14 [Executor task launch worker for task 38.0 in stage 1.0 (TID 39)] INFO  Executor:60 - Running task 38.0 in stage 1.0 (TID 39)
2024-09-18 06:00:14 [Executor task launch worker for task 38.0 in stage 1.0 (TID 39)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5100273664-5234491392, partition values: [empty row]
2024-09-18 06:00:14 [Executor task launch worker for task 35.0 in stage 1.0 (TID 36)] INFO  Executor:60 - Finished task 35.0 in stage 1.0 (TID 36). 1800 bytes result sent to driver
2024-09-18 06:00:14 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 39.0 in stage 1.0 (TID 40) (nn, executor driver, partition 39, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:14 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 35.0 in stage 1.0 (TID 36) in 3325 ms on nn (executor driver) (36/43)
2024-09-18 06:00:14 [Executor task launch worker for task 39.0 in stage 1.0 (TID 40)] INFO  Executor:60 - Running task 39.0 in stage 1.0 (TID 40)
2024-09-18 06:00:14 [Executor task launch worker for task 39.0 in stage 1.0 (TID 40)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5234491392-5368709120, partition values: [empty row]
2024-09-18 06:00:16 [Executor task launch worker for task 36.0 in stage 1.0 (TID 37)] INFO  Executor:60 - Finished task 36.0 in stage 1.0 (TID 37). 1800 bytes result sent to driver
2024-09-18 06:00:16 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 40.0 in stage 1.0 (TID 41) (nn, executor driver, partition 40, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:16 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 36.0 in stage 1.0 (TID 37) in 3027 ms on nn (executor driver) (37/43)
2024-09-18 06:00:16 [Executor task launch worker for task 40.0 in stage 1.0 (TID 41)] INFO  Executor:60 - Running task 40.0 in stage 1.0 (TID 41)
2024-09-18 06:00:16 [Executor task launch worker for task 40.0 in stage 1.0 (TID 41)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5368709120-5502926848, partition values: [empty row]
2024-09-18 06:00:17 [Executor task launch worker for task 37.0 in stage 1.0 (TID 38)] INFO  Executor:60 - Finished task 37.0 in stage 1.0 (TID 38). 1800 bytes result sent to driver
2024-09-18 06:00:17 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 41.0 in stage 1.0 (TID 42) (nn, executor driver, partition 41, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:17 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 37.0 in stage 1.0 (TID 38) in 3322 ms on nn (executor driver) (38/43)
2024-09-18 06:00:17 [Executor task launch worker for task 41.0 in stage 1.0 (TID 42)] INFO  Executor:60 - Running task 41.0 in stage 1.0 (TID 42)
2024-09-18 06:00:17 [Executor task launch worker for task 41.0 in stage 1.0 (TID 42)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5502926848-5637144576, partition values: [empty row]
2024-09-18 06:00:17 [Executor task launch worker for task 38.0 in stage 1.0 (TID 39)] INFO  Executor:60 - Finished task 38.0 in stage 1.0 (TID 39). 1800 bytes result sent to driver
2024-09-18 06:00:17 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 42.0 in stage 1.0 (TID 43) (nn, executor driver, partition 42, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:17 [Executor task launch worker for task 42.0 in stage 1.0 (TID 43)] INFO  Executor:60 - Running task 42.0 in stage 1.0 (TID 43)
2024-09-18 06:00:17 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 38.0 in stage 1.0 (TID 39) in 3351 ms on nn (executor driver) (39/43)
2024-09-18 06:00:17 [Executor task launch worker for task 42.0 in stage 1.0 (TID 43)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5637144576-5668612855, partition values: [empty row]
2024-09-18 06:00:17 [Executor task launch worker for task 39.0 in stage 1.0 (TID 40)] INFO  Executor:60 - Finished task 39.0 in stage 1.0 (TID 40). 1800 bytes result sent to driver
2024-09-18 06:00:17 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 39.0 in stage 1.0 (TID 40) in 3212 ms on nn (executor driver) (40/43)
2024-09-18 06:00:17 [Executor task launch worker for task 42.0 in stage 1.0 (TID 43)] INFO  Executor:60 - Finished task 42.0 in stage 1.0 (TID 43). 1800 bytes result sent to driver
2024-09-18 06:00:17 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 42.0 in stage 1.0 (TID 43) in 608 ms on nn (executor driver) (41/43)
2024-09-18 06:00:18 [Executor task launch worker for task 40.0 in stage 1.0 (TID 41)] INFO  Executor:60 - Finished task 40.0 in stage 1.0 (TID 41). 1800 bytes result sent to driver
2024-09-18 06:00:18 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 40.0 in stage 1.0 (TID 41) in 2740 ms on nn (executor driver) (42/43)
2024-09-18 06:00:19 [Executor task launch worker for task 41.0 in stage 1.0 (TID 42)] INFO  Executor:60 - Finished task 41.0 in stage 1.0 (TID 42). 1800 bytes result sent to driver
2024-09-18 06:00:19 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 41.0 in stage 1.0 (TID 42) in 2229 ms on nn (executor driver) (43/43)
2024-09-18 06:00:19 [task-result-getter-3] INFO  TaskSchedulerImpl:60 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - ResultStage 1 (load at UserActivityProcessor.java:28) finished in 33.492 s
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Killing all running tasks in stage 1: Stage finished
2024-09-18 06:00:19 [main] INFO  DAGScheduler:60 - Job 1 finished: load at UserActivityProcessor.java:28, took 33.498571 s
2024-09-18 06:00:19 [main] INFO  FileSourceStrategy:60 - Pushed Filters: 
2024-09-18 06:00:19 [main] INFO  FileSourceStrategy:60 - Post-Scan Filters: 
2024-09-18 06:00:19 [main] INFO  ParquetUtils:60 - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [main] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [main] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [main] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [main] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [main] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [main] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [main] INFO  CodeGenerator:60 - Code generated in 42.574208 ms
2024-09-18 06:00:19 [main] INFO  MemoryStore:60 - Block broadcast_4 stored as values in memory (estimated size 352.3 KiB, free 1146.1 MiB)
2024-09-18 06:00:19 [main] INFO  MemoryStore:60 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 1146.0 MiB)
2024-09-18 06:00:19 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_4_piece0 in memory on nn:44681 (size: 34.3 KiB, free: 1147.1 MiB)
2024-09-18 06:00:19 [main] INFO  SparkContext:60 - Created broadcast 4 from save at UserActivityProcessor.java:44
2024-09-18 06:00:19 [main] INFO  FileSourceScanExec:60 - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2024-09-18 06:00:19 [main] INFO  SparkContext:60 - Starting job: save at UserActivityProcessor.java:44
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Got job 2 (save at UserActivityProcessor.java:44) with 43 output partitions
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Final stage: ResultStage 2 (save at UserActivityProcessor.java:44)
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Parents of final stage: List()
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Missing parents: List()
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting ResultStage 2 (MapPartitionsRDD[13] at save at UserActivityProcessor.java:44), which has no missing parents
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_5 stored as values in memory (estimated size 238.5 KiB, free 1145.8 MiB)
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  MemoryStore:60 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 87.6 KiB, free 1145.7 MiB)
2024-09-18 06:00:19 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Added broadcast_5_piece0 in memory on nn:44681 (size: 87.6 KiB, free: 1147.0 MiB)
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  SparkContext:60 - Created broadcast 5 from broadcast at DAGScheduler.scala:1585
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Submitting 43 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at save at UserActivityProcessor.java:44) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2024-09-18 06:00:19 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Adding task set 2.0 with 43 tasks resource profile 0
2024-09-18 06:00:19 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 0.0 in stage 2.0 (TID 44) (nn, executor driver, partition 0, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:19 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 1.0 in stage 2.0 (TID 45) (nn, executor driver, partition 1, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:19 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 2.0 in stage 2.0 (TID 46) (nn, executor driver, partition 2, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:19 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 3.0 in stage 2.0 (TID 47) (nn, executor driver, partition 3, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  Executor:60 - Running task 1.0 in stage 2.0 (TID 45)
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  Executor:60 - Running task 0.0 in stage 2.0 (TID 44)
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  Executor:60 - Running task 3.0 in stage 2.0 (TID 47)
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  Executor:60 - Running task 2.0 in stage 2.0 (TID 46)
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodeGenerator:60 - Code generated in 20.229625 ms
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  CodeGenerator:60 - Code generated in 10.717334 ms
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodeGenerator:60 - Code generated in 5.415125 ms
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 134217728-268435456, partition values: [empty row]
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 402653184-536870912, partition values: [empty row]
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:19 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 268435456-402653184, partition values: [empty row]
2024-09-18 06:00:19 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodeGenerator:60 - Code generated in 15.670292 ms
2024-09-18 06:00:20 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodeGenerator:60 - Code generated in 15.756125 ms
2024-09-18 06:00:20 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 0-134217728, partition values: [empty row]
2024-09-18 06:00:20 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Removed broadcast_3_piece0 on nn:44681 in memory (size: 12.7 KiB, free: 1147.0 MiB)
2024-09-18 06:00:20 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Removed broadcast_0_piece0 on nn:44681 in memory (size: 34.2 KiB, free: 1147.0 MiB)
2024-09-18 06:00:20 [dispatcher-BlockManagerMaster] INFO  BlockManagerInfo:60 - Removed broadcast_2_piece0 on nn:44681 in memory (size: 34.2 KiB, free: 1147.1 MiB)
2024-09-18 06:00:25 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodeGenerator:60 - Code generated in 64.555375 ms
2024-09-18 06:00:25 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodeGenerator:60 - Code generated in 79.128 ms
2024-09-18 06:00:25 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:25 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:25 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:25 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:26 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:26 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:26 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:26 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:26 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:26 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:26 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:26 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:26 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:26 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:26 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:26 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:26 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  CodecPool:153 - Got brand-new compressor [.snappy]
2024-09-18 06:00:26 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodecPool:153 - Got brand-new compressor [.snappy]
2024-09-18 06:00:26 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodecPool:153 - Got brand-new compressor [.snappy]
2024-09-18 06:00:26 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  CodecPool:153 - Got brand-new compressor [.snappy]
2024-09-18 06:00:27 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:27 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:27 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:27 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:27 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:27 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:27 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:27 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:29 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000001_45' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000001
2024-09-18 06:00:29 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000001_45: Committed. Elapsed time: 16 ms.
2024-09-18 06:00:29 [Executor task launch worker for task 1.0 in stage 2.0 (TID 45)] INFO  Executor:60 - Finished task 1.0 in stage 2.0 (TID 45). 3355 bytes result sent to driver
2024-09-18 06:00:29 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 4.0 in stage 2.0 (TID 48) (nn, executor driver, partition 4, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:29 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 1.0 in stage 2.0 (TID 45) in 9758 ms on nn (executor driver) (1/43)
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  Executor:60 - Running task 4.0 in stage 2.0 (TID 48)
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:29 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 536870912-671088640, partition values: [empty row]
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000000_44' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000000
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000000_44: Committed. Elapsed time: 4 ms.
2024-09-18 06:00:29 [Executor task launch worker for task 0.0 in stage 2.0 (TID 44)] INFO  Executor:60 - Finished task 0.0 in stage 2.0 (TID 44). 3400 bytes result sent to driver
2024-09-18 06:00:29 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 5.0 in stage 2.0 (TID 49) (nn, executor driver, partition 5, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:29 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 0.0 in stage 2.0 (TID 44) in 9946 ms on nn (executor driver) (2/43)
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  Executor:60 - Running task 5.0 in stage 2.0 (TID 49)
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:29 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 671088640-805306368, partition values: [empty row]
2024-09-18 06:00:29 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000003_47' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000003
2024-09-18 06:00:29 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000003_47: Committed. Elapsed time: 23 ms.
2024-09-18 06:00:29 [Executor task launch worker for task 3.0 in stage 2.0 (TID 47)] INFO  Executor:60 - Finished task 3.0 in stage 2.0 (TID 47). 3400 bytes result sent to driver
2024-09-18 06:00:29 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 6.0 in stage 2.0 (TID 50) (nn, executor driver, partition 6, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  Executor:60 - Running task 6.0 in stage 2.0 (TID 50)
2024-09-18 06:00:29 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 3.0 in stage 2.0 (TID 47) in 10000 ms on nn (executor driver) (3/43)
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:29 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 805306368-939524096, partition values: [empty row]
2024-09-18 06:00:30 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000002_46' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000002
2024-09-18 06:00:30 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000002_46: Committed. Elapsed time: 4 ms.
2024-09-18 06:00:30 [Executor task launch worker for task 2.0 in stage 2.0 (TID 46)] INFO  Executor:60 - Finished task 2.0 in stage 2.0 (TID 46). 3400 bytes result sent to driver
2024-09-18 06:00:30 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 7.0 in stage 2.0 (TID 51) (nn, executor driver, partition 7, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:30 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 2.0 in stage 2.0 (TID 46) in 10581 ms on nn (executor driver) (4/43)
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  Executor:60 - Running task 7.0 in stage 2.0 (TID 51)
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:30 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 939524096-1073741824, partition values: [empty row]
2024-09-18 06:00:34 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:34 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:34 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:34 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:35 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:35 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:35 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:35 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:35 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:35 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:35 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:35 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:36 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:36 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:36 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:36 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:36 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:37 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:37 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:37 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:37 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:38 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000006_50' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000006
2024-09-18 06:00:38 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000006_50: Committed. Elapsed time: 8 ms.
2024-09-18 06:00:38 [Executor task launch worker for task 6.0 in stage 2.0 (TID 50)] INFO  Executor:60 - Finished task 6.0 in stage 2.0 (TID 50). 3312 bytes result sent to driver
2024-09-18 06:00:38 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 8.0 in stage 2.0 (TID 52) (nn, executor driver, partition 8, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  Executor:60 - Running task 8.0 in stage 2.0 (TID 52)
2024-09-18 06:00:38 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 6.0 in stage 2.0 (TID 50) in 8196 ms on nn (executor driver) (5/43)
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:38 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1073741824-1207959552, partition values: [empty row]
2024-09-18 06:00:38 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000005_49' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000005
2024-09-18 06:00:38 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000005_49: Committed. Elapsed time: 42 ms.
2024-09-18 06:00:38 [Executor task launch worker for task 5.0 in stage 2.0 (TID 49)] INFO  Executor:60 - Finished task 5.0 in stage 2.0 (TID 49). 3400 bytes result sent to driver
2024-09-18 06:00:38 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 9.0 in stage 2.0 (TID 53) (nn, executor driver, partition 9, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:38 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000004_48' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000004
2024-09-18 06:00:38 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 5.0 in stage 2.0 (TID 49) in 8535 ms on nn (executor driver) (6/43)
2024-09-18 06:00:38 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000004_48: Committed. Elapsed time: 9 ms.
2024-09-18 06:00:38 [Executor task launch worker for task 4.0 in stage 2.0 (TID 48)] INFO  Executor:60 - Finished task 4.0 in stage 2.0 (TID 48). 3400 bytes result sent to driver
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  Executor:60 - Running task 9.0 in stage 2.0 (TID 53)
2024-09-18 06:00:38 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 10.0 in stage 2.0 (TID 54) (nn, executor driver, partition 10, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  Executor:60 - Running task 10.0 in stage 2.0 (TID 54)
2024-09-18 06:00:38 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 4.0 in stage 2.0 (TID 48) in 8726 ms on nn (executor driver) (7/43)
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:38 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1342177280-1476395008, partition values: [empty row]
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:38 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1207959552-1342177280, partition values: [empty row]
2024-09-18 06:00:39 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000007_51' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000007
2024-09-18 06:00:39 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000007_51: Committed. Elapsed time: 9 ms.
2024-09-18 06:00:39 [Executor task launch worker for task 7.0 in stage 2.0 (TID 51)] INFO  Executor:60 - Finished task 7.0 in stage 2.0 (TID 51). 3400 bytes result sent to driver
2024-09-18 06:00:39 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 11.0 in stage 2.0 (TID 55) (nn, executor driver, partition 11, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  Executor:60 - Running task 11.0 in stage 2.0 (TID 55)
2024-09-18 06:00:39 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 7.0 in stage 2.0 (TID 51) in 9208 ms on nn (executor driver) (8/43)
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:39 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1476395008-1610612736, partition values: [empty row]
2024-09-18 06:00:43 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:44 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:44 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:44 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:44 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:44 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:44 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:46 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:46 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:46 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:46 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:47 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:47 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:47 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:47 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:48 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000010_54' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000010
2024-09-18 06:00:48 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000010_54: Committed. Elapsed time: 5 ms.
2024-09-18 06:00:48 [Executor task launch worker for task 10.0 in stage 2.0 (TID 54)] INFO  Executor:60 - Finished task 10.0 in stage 2.0 (TID 54). 3312 bytes result sent to driver
2024-09-18 06:00:48 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 12.0 in stage 2.0 (TID 56) (nn, executor driver, partition 12, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  Executor:60 - Running task 12.0 in stage 2.0 (TID 56)
2024-09-18 06:00:48 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 10.0 in stage 2.0 (TID 54) in 9854 ms on nn (executor driver) (9/43)
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:48 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1610612736-1744830464, partition values: [empty row]
2024-09-18 06:00:48 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000009_53' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000009
2024-09-18 06:00:48 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000009_53: Committed. Elapsed time: 12 ms.
2024-09-18 06:00:48 [Executor task launch worker for task 9.0 in stage 2.0 (TID 53)] INFO  Executor:60 - Finished task 9.0 in stage 2.0 (TID 53). 3400 bytes result sent to driver
2024-09-18 06:00:48 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 13.0 in stage 2.0 (TID 57) (nn, executor driver, partition 13, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  Executor:60 - Running task 13.0 in stage 2.0 (TID 57)
2024-09-18 06:00:48 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 9.0 in stage 2.0 (TID 53) in 9927 ms on nn (executor driver) (10/43)
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:48 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1744830464-1879048192, partition values: [empty row]
2024-09-18 06:00:48 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000011_55' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000011
2024-09-18 06:00:48 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000011_55: Committed. Elapsed time: 8 ms.
2024-09-18 06:00:48 [Executor task launch worker for task 11.0 in stage 2.0 (TID 55)] INFO  Executor:60 - Finished task 11.0 in stage 2.0 (TID 55). 3401 bytes result sent to driver
2024-09-18 06:00:48 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 14.0 in stage 2.0 (TID 58) (nn, executor driver, partition 14, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  Executor:60 - Running task 14.0 in stage 2.0 (TID 58)
2024-09-18 06:00:48 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 11.0 in stage 2.0 (TID 55) in 9281 ms on nn (executor driver) (11/43)
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:48 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 1879048192-2013265920, partition values: [empty row]
2024-09-18 06:00:49 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000008_52' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000008
2024-09-18 06:00:49 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000008_52: Committed. Elapsed time: 6 ms.
2024-09-18 06:00:49 [Executor task launch worker for task 8.0 in stage 2.0 (TID 52)] INFO  Executor:60 - Finished task 8.0 in stage 2.0 (TID 52). 3400 bytes result sent to driver
2024-09-18 06:00:49 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 15.0 in stage 2.0 (TID 59) (nn, executor driver, partition 15, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  Executor:60 - Running task 15.0 in stage 2.0 (TID 59)
2024-09-18 06:00:49 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 8.0 in stage 2.0 (TID 52) in 11455 ms on nn (executor driver) (12/43)
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:49 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2013265920-2147483648, partition values: [empty row]
2024-09-18 06:00:52 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:52 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:52 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:53 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:53 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:53 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:53 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:53 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:53 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:53 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:53 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:53 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:54 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:54 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:54 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:54 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:54 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:54 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:54 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:54 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:55 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:55 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:00:55 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:00:55 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000013_57' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000013
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000013_57: Committed. Elapsed time: 2 ms.
2024-09-18 06:00:55 [Executor task launch worker for task 13.0 in stage 2.0 (TID 57)] INFO  Executor:60 - Finished task 13.0 in stage 2.0 (TID 57). 3402 bytes result sent to driver
2024-09-18 06:00:55 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 16.0 in stage 2.0 (TID 60) (nn, executor driver, partition 16, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  Executor:60 - Running task 16.0 in stage 2.0 (TID 60)
2024-09-18 06:00:55 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 13.0 in stage 2.0 (TID 57) in 7541 ms on nn (executor driver) (13/43)
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:55 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2147483648-2281701376, partition values: [empty row]
2024-09-18 06:00:56 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000014_58' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000014
2024-09-18 06:00:56 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000014_58: Committed. Elapsed time: 1 ms.
2024-09-18 06:00:56 [Executor task launch worker for task 14.0 in stage 2.0 (TID 58)] INFO  Executor:60 - Finished task 14.0 in stage 2.0 (TID 58). 3313 bytes result sent to driver
2024-09-18 06:00:56 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 17.0 in stage 2.0 (TID 61) (nn, executor driver, partition 17, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:56 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 14.0 in stage 2.0 (TID 58) in 7184 ms on nn (executor driver) (14/43)
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  Executor:60 - Running task 17.0 in stage 2.0 (TID 61)
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:56 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2281701376-2415919104, partition values: [empty row]
2024-09-18 06:00:56 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000012_56' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000012
2024-09-18 06:00:56 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000012_56: Committed. Elapsed time: 6 ms.
2024-09-18 06:00:56 [Executor task launch worker for task 12.0 in stage 2.0 (TID 56)] INFO  Executor:60 - Finished task 12.0 in stage 2.0 (TID 56). 3402 bytes result sent to driver
2024-09-18 06:00:56 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 18.0 in stage 2.0 (TID 62) (nn, executor driver, partition 18, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  Executor:60 - Running task 18.0 in stage 2.0 (TID 62)
2024-09-18 06:00:56 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 12.0 in stage 2.0 (TID 56) in 8190 ms on nn (executor driver) (15/43)
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:56 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2415919104-2550136832, partition values: [empty row]
2024-09-18 06:00:59 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000015_59' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000015
2024-09-18 06:00:59 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000015_59: Committed. Elapsed time: 8 ms.
2024-09-18 06:00:59 [Executor task launch worker for task 15.0 in stage 2.0 (TID 59)] INFO  Executor:60 - Finished task 15.0 in stage 2.0 (TID 59). 3402 bytes result sent to driver
2024-09-18 06:00:59 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 19.0 in stage 2.0 (TID 63) (nn, executor driver, partition 19, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:00:59 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 15.0 in stage 2.0 (TID 59) in 10213 ms on nn (executor driver) (16/43)
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  Executor:60 - Running task 19.0 in stage 2.0 (TID 63)
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:00:59 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2550136832-2684354560, partition values: [empty row]
2024-09-18 06:01:02 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:02 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:02 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:02 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:02 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:02 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:02 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:02 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:02 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:02 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:02 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:02 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:03 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:03 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:03 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:03 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:04 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:04 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:04 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:04 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:04 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:04 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:04 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:04 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:05 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000017_61' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000017
2024-09-18 06:01:05 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000017_61: Committed. Elapsed time: 7 ms.
2024-09-18 06:01:05 [Executor task launch worker for task 17.0 in stage 2.0 (TID 61)] INFO  Executor:60 - Finished task 17.0 in stage 2.0 (TID 61). 3313 bytes result sent to driver
2024-09-18 06:01:05 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 20.0 in stage 2.0 (TID 64) (nn, executor driver, partition 20, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  Executor:60 - Running task 20.0 in stage 2.0 (TID 64)
2024-09-18 06:01:05 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 17.0 in stage 2.0 (TID 61) in 9034 ms on nn (executor driver) (17/43)
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:05 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2684354560-2818572288, partition values: [empty row]
2024-09-18 06:01:05 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000018_62' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000018
2024-09-18 06:01:05 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000018_62: Committed. Elapsed time: 8 ms.
2024-09-18 06:01:05 [Executor task launch worker for task 18.0 in stage 2.0 (TID 62)] INFO  Executor:60 - Finished task 18.0 in stage 2.0 (TID 62). 3402 bytes result sent to driver
2024-09-18 06:01:05 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 21.0 in stage 2.0 (TID 65) (nn, executor driver, partition 21, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:05 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 18.0 in stage 2.0 (TID 62) in 8826 ms on nn (executor driver) (18/43)
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  Executor:60 - Running task 21.0 in stage 2.0 (TID 65)
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:05 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2818572288-2952790016, partition values: [empty row]
2024-09-18 06:01:05 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000016_60' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000016
2024-09-18 06:01:05 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000016_60: Committed. Elapsed time: 2 ms.
2024-09-18 06:01:05 [Executor task launch worker for task 16.0 in stage 2.0 (TID 60)] INFO  Executor:60 - Finished task 16.0 in stage 2.0 (TID 60). 3402 bytes result sent to driver
2024-09-18 06:01:05 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 22.0 in stage 2.0 (TID 66) (nn, executor driver, partition 22, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  Executor:60 - Running task 22.0 in stage 2.0 (TID 66)
2024-09-18 06:01:05 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 16.0 in stage 2.0 (TID 60) in 9660 ms on nn (executor driver) (19/43)
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:05 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 2952790016-3087007744, partition values: [empty row]
2024-09-18 06:01:07 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:07 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:07 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:07 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:08 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000019_63' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000019
2024-09-18 06:01:08 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000019_63: Committed. Elapsed time: 5 ms.
2024-09-18 06:01:08 [Executor task launch worker for task 19.0 in stage 2.0 (TID 63)] INFO  Executor:60 - Finished task 19.0 in stage 2.0 (TID 63). 3402 bytes result sent to driver
2024-09-18 06:01:08 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 23.0 in stage 2.0 (TID 67) (nn, executor driver, partition 23, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  Executor:60 - Running task 23.0 in stage 2.0 (TID 67)
2024-09-18 06:01:08 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 19.0 in stage 2.0 (TID 63) in 8424 ms on nn (executor driver) (20/43)
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:08 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3087007744-3221225472, partition values: [empty row]
2024-09-18 06:01:09 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:09 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:09 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:09 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:09 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:09 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:09 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:09 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:10 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:10 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:10 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:10 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:10 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:10 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:10 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:10 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:11 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000020_64' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000020
2024-09-18 06:01:11 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000020_64: Committed. Elapsed time: 20 ms.
2024-09-18 06:01:11 [Executor task launch worker for task 20.0 in stage 2.0 (TID 64)] INFO  Executor:60 - Finished task 20.0 in stage 2.0 (TID 64). 3356 bytes result sent to driver
2024-09-18 06:01:11 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 24.0 in stage 2.0 (TID 68) (nn, executor driver, partition 24, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  Executor:60 - Running task 24.0 in stage 2.0 (TID 68)
2024-09-18 06:01:11 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 20.0 in stage 2.0 (TID 64) in 6779 ms on nn (executor driver) (21/43)
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:11 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3221225472-3355443200, partition values: [empty row]
2024-09-18 06:01:12 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:12 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:12 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:12 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:12 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000021_65' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000021
2024-09-18 06:01:12 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000021_65: Committed. Elapsed time: 6 ms.
2024-09-18 06:01:12 [Executor task launch worker for task 21.0 in stage 2.0 (TID 65)] INFO  Executor:60 - Finished task 21.0 in stage 2.0 (TID 65). 3402 bytes result sent to driver
2024-09-18 06:01:12 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 25.0 in stage 2.0 (TID 69) (nn, executor driver, partition 25, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:12 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 21.0 in stage 2.0 (TID 65) in 7043 ms on nn (executor driver) (22/43)
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  Executor:60 - Running task 25.0 in stage 2.0 (TID 69)
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:12 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3355443200-3489660928, partition values: [empty row]
2024-09-18 06:01:12 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:12 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:12 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:12 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:13 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000022_66' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000022
2024-09-18 06:01:13 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000022_66: Committed. Elapsed time: 8 ms.
2024-09-18 06:01:13 [Executor task launch worker for task 22.0 in stage 2.0 (TID 66)] INFO  Executor:60 - Finished task 22.0 in stage 2.0 (TID 66). 3402 bytes result sent to driver
2024-09-18 06:01:13 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 26.0 in stage 2.0 (TID 70) (nn, executor driver, partition 26, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:13 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 22.0 in stage 2.0 (TID 66) in 7684 ms on nn (executor driver) (23/43)
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  Executor:60 - Running task 26.0 in stage 2.0 (TID 70)
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:13 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3489660928-3623878656, partition values: [empty row]
2024-09-18 06:01:15 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000023_67' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000023
2024-09-18 06:01:15 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000023_67: Committed. Elapsed time: 3 ms.
2024-09-18 06:01:15 [Executor task launch worker for task 23.0 in stage 2.0 (TID 67)] INFO  Executor:60 - Finished task 23.0 in stage 2.0 (TID 67). 3313 bytes result sent to driver
2024-09-18 06:01:15 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 27.0 in stage 2.0 (TID 71) (nn, executor driver, partition 27, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:15 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 23.0 in stage 2.0 (TID 67) in 7417 ms on nn (executor driver) (24/43)
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  Executor:60 - Running task 27.0 in stage 2.0 (TID 71)
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:15 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3623878656-3758096384, partition values: [empty row]
2024-09-18 06:01:16 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:16 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:16 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:16 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:16 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:16 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:16 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:16 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:17 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:17 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:17 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:17 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:17 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:17 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:17 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:17 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:18 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:18 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:18 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:18 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:19 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000025_69' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000025
2024-09-18 06:01:19 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000025_69: Committed. Elapsed time: 9 ms.
2024-09-18 06:01:19 [Executor task launch worker for task 25.0 in stage 2.0 (TID 69)] INFO  Executor:60 - Finished task 25.0 in stage 2.0 (TID 69). 3402 bytes result sent to driver
2024-09-18 06:01:19 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 28.0 in stage 2.0 (TID 72) (nn, executor driver, partition 28, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  Executor:60 - Running task 28.0 in stage 2.0 (TID 72)
2024-09-18 06:01:19 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 25.0 in stage 2.0 (TID 69) in 7152 ms on nn (executor driver) (25/43)
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:19 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3758096384-3892314112, partition values: [empty row]
2024-09-18 06:01:19 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000024_68' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000024
2024-09-18 06:01:19 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000024_68: Committed. Elapsed time: 3 ms.
2024-09-18 06:01:19 [Executor task launch worker for task 24.0 in stage 2.0 (TID 68)] INFO  Executor:60 - Finished task 24.0 in stage 2.0 (TID 68). 3402 bytes result sent to driver
2024-09-18 06:01:19 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 29.0 in stage 2.0 (TID 73) (nn, executor driver, partition 29, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  Executor:60 - Running task 29.0 in stage 2.0 (TID 73)
2024-09-18 06:01:19 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 24.0 in stage 2.0 (TID 68) in 7942 ms on nn (executor driver) (26/43)
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:19 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 3892314112-4026531840, partition values: [empty row]
2024-09-18 06:01:20 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:20 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:20 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:20 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:20 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000026_70' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000026
2024-09-18 06:01:20 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000026_70: Committed. Elapsed time: 6 ms.
2024-09-18 06:01:20 [Executor task launch worker for task 26.0 in stage 2.0 (TID 70)] INFO  Executor:60 - Finished task 26.0 in stage 2.0 (TID 70). 3313 bytes result sent to driver
2024-09-18 06:01:20 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 30.0 in stage 2.0 (TID 74) (nn, executor driver, partition 30, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:20 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 26.0 in stage 2.0 (TID 70) in 7793 ms on nn (executor driver) (27/43)
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  Executor:60 - Running task 30.0 in stage 2.0 (TID 74)
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:20 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4026531840-4160749568, partition values: [empty row]
2024-09-18 06:01:21 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:21 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:21 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:21 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:23 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000027_71' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000027
2024-09-18 06:01:23 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000027_71: Committed. Elapsed time: 12 ms.
2024-09-18 06:01:23 [Executor task launch worker for task 27.0 in stage 2.0 (TID 71)] INFO  Executor:60 - Finished task 27.0 in stage 2.0 (TID 71). 3402 bytes result sent to driver
2024-09-18 06:01:23 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 31.0 in stage 2.0 (TID 75) (nn, executor driver, partition 31, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  Executor:60 - Running task 31.0 in stage 2.0 (TID 75)
2024-09-18 06:01:23 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 27.0 in stage 2.0 (TID 71) in 8109 ms on nn (executor driver) (28/43)
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:23 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4160749568-4294967296, partition values: [empty row]
2024-09-18 06:01:24 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:24 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:24 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:24 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:24 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:24 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:24 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:24 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:25 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:25 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:25 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:25 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:25 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:26 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000028_72' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000028
2024-09-18 06:01:26 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000028_72: Committed. Elapsed time: 2 ms.
2024-09-18 06:01:26 [Executor task launch worker for task 28.0 in stage 2.0 (TID 72)] INFO  Executor:60 - Finished task 28.0 in stage 2.0 (TID 72). 3402 bytes result sent to driver
2024-09-18 06:01:26 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 32.0 in stage 2.0 (TID 76) (nn, executor driver, partition 32, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  Executor:60 - Running task 32.0 in stage 2.0 (TID 76)
2024-09-18 06:01:26 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 28.0 in stage 2.0 (TID 72) in 7095 ms on nn (executor driver) (29/43)
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:26 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4294967296-4429185024, partition values: [empty row]
2024-09-18 06:01:27 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000029_73' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000029
2024-09-18 06:01:27 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000029_73: Committed. Elapsed time: 5 ms.
2024-09-18 06:01:27 [Executor task launch worker for task 29.0 in stage 2.0 (TID 73)] INFO  Executor:60 - Finished task 29.0 in stage 2.0 (TID 73). 3313 bytes result sent to driver
2024-09-18 06:01:27 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 33.0 in stage 2.0 (TID 77) (nn, executor driver, partition 33, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:27 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 29.0 in stage 2.0 (TID 73) in 7284 ms on nn (executor driver) (30/43)
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  Executor:60 - Running task 33.0 in stage 2.0 (TID 77)
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:27 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4429185024-4563402752, partition values: [empty row]
2024-09-18 06:01:28 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:28 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:28 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:28 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:28 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000030_74' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000030
2024-09-18 06:01:28 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000030_74: Committed. Elapsed time: 3 ms.
2024-09-18 06:01:28 [Executor task launch worker for task 30.0 in stage 2.0 (TID 74)] INFO  Executor:60 - Finished task 30.0 in stage 2.0 (TID 74). 3402 bytes result sent to driver
2024-09-18 06:01:28 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 34.0 in stage 2.0 (TID 78) (nn, executor driver, partition 34, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:28 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 30.0 in stage 2.0 (TID 74) in 7826 ms on nn (executor driver) (31/43)
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  Executor:60 - Running task 34.0 in stage 2.0 (TID 78)
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:28 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4563402752-4697620480, partition values: [empty row]
2024-09-18 06:01:29 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:29 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:29 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:29 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:30 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:30 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:30 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:30 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:31 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:31 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:31 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:31 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:31 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000031_75' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000031
2024-09-18 06:01:31 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000031_75: Committed. Elapsed time: 3 ms.
2024-09-18 06:01:31 [Executor task launch worker for task 31.0 in stage 2.0 (TID 75)] INFO  Executor:60 - Finished task 31.0 in stage 2.0 (TID 75). 3402 bytes result sent to driver
2024-09-18 06:01:31 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 35.0 in stage 2.0 (TID 79) (nn, executor driver, partition 35, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:31 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 31.0 in stage 2.0 (TID 75) in 7825 ms on nn (executor driver) (32/43)
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  Executor:60 - Running task 35.0 in stage 2.0 (TID 79)
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:31 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4697620480-4831838208, partition values: [empty row]
2024-09-18 06:01:32 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:32 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:32 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:32 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:33 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:33 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:33 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:33 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000032_76' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000032
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000032_76: Committed. Elapsed time: 13 ms.
2024-09-18 06:01:33 [Executor task launch worker for task 32.0 in stage 2.0 (TID 76)] INFO  Executor:60 - Finished task 32.0 in stage 2.0 (TID 76). 3402 bytes result sent to driver
2024-09-18 06:01:33 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 36.0 in stage 2.0 (TID 80) (nn, executor driver, partition 36, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:33 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  Executor:60 - Running task 36.0 in stage 2.0 (TID 80)
2024-09-18 06:01:33 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 32.0 in stage 2.0 (TID 76) in 7534 ms on nn (executor driver) (33/43)
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:34 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4831838208-4966055936, partition values: [empty row]
2024-09-18 06:01:34 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000033_77' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000033
2024-09-18 06:01:34 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000033_77: Committed. Elapsed time: 6 ms.
2024-09-18 06:01:34 [Executor task launch worker for task 33.0 in stage 2.0 (TID 77)] INFO  Executor:60 - Finished task 33.0 in stage 2.0 (TID 77). 3313 bytes result sent to driver
2024-09-18 06:01:34 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 37.0 in stage 2.0 (TID 81) (nn, executor driver, partition 37, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  Executor:60 - Running task 37.0 in stage 2.0 (TID 81)
2024-09-18 06:01:34 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 33.0 in stage 2.0 (TID 77) in 7515 ms on nn (executor driver) (34/43)
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:34 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 4966055936-5100273664, partition values: [empty row]
2024-09-18 06:01:36 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000034_78' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000034
2024-09-18 06:01:36 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000034_78: Committed. Elapsed time: 7 ms.
2024-09-18 06:01:36 [Executor task launch worker for task 34.0 in stage 2.0 (TID 78)] INFO  Executor:60 - Finished task 34.0 in stage 2.0 (TID 78). 3445 bytes result sent to driver
2024-09-18 06:01:36 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 38.0 in stage 2.0 (TID 82) (nn, executor driver, partition 38, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:36 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 34.0 in stage 2.0 (TID 78) in 7393 ms on nn (executor driver) (35/43)
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  Executor:60 - Running task 38.0 in stage 2.0 (TID 82)
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:36 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5100273664-5234491392, partition values: [empty row]
2024-09-18 06:01:36 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:36 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:36 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:36 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:37 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:37 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:37 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:37 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:39 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:39 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:39 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:39 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:39 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:39 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:39 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:39 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:40 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000035_79' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000035
2024-09-18 06:01:40 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000035_79: Committed. Elapsed time: 7 ms.
2024-09-18 06:01:40 [Executor task launch worker for task 35.0 in stage 2.0 (TID 79)] INFO  Executor:60 - Finished task 35.0 in stage 2.0 (TID 79). 3402 bytes result sent to driver
2024-09-18 06:01:40 [dispatcher-event-loop-2] INFO  TaskSetManager:60 - Starting task 39.0 in stage 2.0 (TID 83) (nn, executor driver, partition 39, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  Executor:60 - Running task 39.0 in stage 2.0 (TID 83)
2024-09-18 06:01:40 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 35.0 in stage 2.0 (TID 79) in 8764 ms on nn (executor driver) (36/43)
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:40 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5234491392-5368709120, partition values: [empty row]
2024-09-18 06:01:41 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:41 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:41 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:41 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:41 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:41 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:41 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:41 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:42 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:42 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:42 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:42 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:42 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000036_80' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000036
2024-09-18 06:01:42 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000036_80: Committed. Elapsed time: 21 ms.
2024-09-18 06:01:42 [Executor task launch worker for task 36.0 in stage 2.0 (TID 80)] INFO  Executor:60 - Finished task 36.0 in stage 2.0 (TID 80). 3402 bytes result sent to driver
2024-09-18 06:01:42 [dispatcher-event-loop-1] INFO  TaskSetManager:60 - Starting task 40.0 in stage 2.0 (TID 84) (nn, executor driver, partition 40, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:42 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 36.0 in stage 2.0 (TID 80) in 8554 ms on nn (executor driver) (37/43)
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  Executor:60 - Running task 40.0 in stage 2.0 (TID 84)
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:42 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5368709120-5502926848, partition values: [empty row]
2024-09-18 06:01:42 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000037_81' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000037
2024-09-18 06:01:42 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000037_81: Committed. Elapsed time: 2 ms.
2024-09-18 06:01:42 [Executor task launch worker for task 37.0 in stage 2.0 (TID 81)] INFO  Executor:60 - Finished task 37.0 in stage 2.0 (TID 81). 3313 bytes result sent to driver
2024-09-18 06:01:42 [dispatcher-event-loop-0] INFO  TaskSetManager:60 - Starting task 41.0 in stage 2.0 (TID 85) (nn, executor driver, partition 41, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  Executor:60 - Running task 41.0 in stage 2.0 (TID 85)
2024-09-18 06:01:42 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 37.0 in stage 2.0 (TID 81) in 8253 ms on nn (executor driver) (38/43)
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:42 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5502926848-5637144576, partition values: [empty row]
2024-09-18 06:01:44 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000038_82' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000038
2024-09-18 06:01:44 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000038_82: Committed. Elapsed time: 4 ms.
2024-09-18 06:01:44 [Executor task launch worker for task 38.0 in stage 2.0 (TID 82)] INFO  Executor:60 - Finished task 38.0 in stage 2.0 (TID 82). 3402 bytes result sent to driver
2024-09-18 06:01:44 [dispatcher-event-loop-3] INFO  TaskSetManager:60 - Starting task 42.0 in stage 2.0 (TID 86) (nn, executor driver, partition 42, PROCESS_LOCAL, 9599 bytes) 
2024-09-18 06:01:44 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 38.0 in stage 2.0 (TID 82) in 8690 ms on nn (executor driver) (39/43)
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  Executor:60 - Running task 42.0 in stage 2.0 (TID 86)
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  FileOutputCommitter:142 - File Output Committer Algorithm version is 1
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  FileOutputCommitter:157 - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  SQLHadoopMapReduceCommitProtocol:60 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2024-09-18 06:01:44 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  FileScanRDD:60 - Reading File path: file:///home/project/data/2019-10.csv, range: 5637144576-5668612855, partition values: [empty row]
2024-09-18 06:01:45 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:45 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:45 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:45 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:45 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:45 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:45 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:45 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:46 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:46 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:46 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:46 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:46 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000042_86' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000042
2024-09-18 06:01:46 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000042_86: Committed. Elapsed time: 8 ms.
2024-09-18 06:01:46 [Executor task launch worker for task 42.0 in stage 2.0 (TID 86)] INFO  Executor:60 - Finished task 42.0 in stage 2.0 (TID 86). 3312 bytes result sent to driver
2024-09-18 06:01:46 [task-result-getter-3] INFO  TaskSetManager:60 - Finished task 42.0 in stage 2.0 (TID 86) in 1510 ms on nn (executor driver) (40/43)
2024-09-18 06:01:47 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:47 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:47 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:47 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:47 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:47 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:47 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:47 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:47 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000039_83' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000039
2024-09-18 06:01:47 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000039_83: Committed. Elapsed time: 8 ms.
2024-09-18 06:01:47 [Executor task launch worker for task 39.0 in stage 2.0 (TID 83)] INFO  Executor:60 - Finished task 39.0 in stage 2.0 (TID 83). 3402 bytes result sent to driver
2024-09-18 06:01:47 [task-result-getter-0] INFO  TaskSetManager:60 - Finished task 39.0 in stage 2.0 (TID 83) in 7557 ms on nn (executor driver) (41/43)
2024-09-18 06:01:48 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:48 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:48 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:48 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:48 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:48 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  CodecConfig:95 - Compression: SNAPPY
2024-09-18 06:01:48 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  ParquetOutputFormat:473 - ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
2024-09-18 06:01:48 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  ParquetWriteSupport:60 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "event_time",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "product_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "category_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "price",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "user_session",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "event_time_kst",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 event_time;
  optional binary event_type (STRING);
  optional int32 product_id;
  optional int64 category_id;
  optional binary category_code (STRING);
  optional binary brand (STRING);
  optional double price;
  optional int32 user_id;
  optional binary user_session (STRING);
  optional int96 event_time_kst;
}

       
2024-09-18 06:01:49 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000041_85' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000041
2024-09-18 06:01:49 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000041_85: Committed. Elapsed time: 1 ms.
2024-09-18 06:01:49 [Executor task launch worker for task 41.0 in stage 2.0 (TID 85)] INFO  Executor:60 - Finished task 41.0 in stage 2.0 (TID 85). 3401 bytes result sent to driver
2024-09-18 06:01:49 [task-result-getter-1] INFO  TaskSetManager:60 - Finished task 41.0 in stage 2.0 (TID 85) in 6376 ms on nn (executor driver) (42/43)
2024-09-18 06:01:49 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  FileOutputCommitter:604 - Saved output of task 'attempt_202409180600198633961678860332670_0002_m_000040_84' to hdfs://nn:9000/user/hive/external/useractivity/_temporary/0/task_202409180600198633961678860332670_0002_m_000040
2024-09-18 06:01:49 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  SparkHadoopMapRedUtil:60 - attempt_202409180600198633961678860332670_0002_m_000040_84: Committed. Elapsed time: 4 ms.
2024-09-18 06:01:49 [Executor task launch worker for task 40.0 in stage 2.0 (TID 84)] INFO  Executor:60 - Finished task 40.0 in stage 2.0 (TID 84). 3402 bytes result sent to driver
2024-09-18 06:01:49 [task-result-getter-2] INFO  TaskSetManager:60 - Finished task 40.0 in stage 2.0 (TID 84) in 7386 ms on nn (executor driver) (43/43)
2024-09-18 06:01:49 [task-result-getter-2] INFO  TaskSchedulerImpl:60 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2024-09-18 06:01:49 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - ResultStage 2 (save at UserActivityProcessor.java:44) finished in 90.130 s
2024-09-18 06:01:49 [dag-scheduler-event-loop] INFO  DAGScheduler:60 - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2024-09-18 06:01:49 [dag-scheduler-event-loop] INFO  TaskSchedulerImpl:60 - Killing all running tasks in stage 2: Stage finished
2024-09-18 06:01:49 [main] INFO  DAGScheduler:60 - Job 2 finished: save at UserActivityProcessor.java:44, took 90.136430 s
2024-09-18 06:01:49 [main] INFO  FileFormatWriter:60 - Start to commit write Job 0e72c121-3da5-4d6d-9285-f5f2a239f496.
2024-09-18 06:01:50 [main] INFO  FileFormatWriter:60 - Write Job 0e72c121-3da5-4d6d-9285-f5f2a239f496 committed. Elapsed time: 263 ms.
2024-09-18 06:01:50 [main] INFO  FileFormatWriter:60 - Finished processing stats for write job 0e72c121-3da5-4d6d-9285-f5f2a239f496.
2024-09-18 06:01:50 [main] INFO  SparkContext:60 - SparkContext is stopping with exitCode 0.
2024-09-18 06:01:50 [main] INFO  AbstractConnector:383 - Stopped Spark@51827393{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-09-18 06:01:50 [main] INFO  SparkUI:60 - Stopped Spark web UI at http://nn:4040
2024-09-18 06:01:50 [dispatcher-event-loop-3] INFO  MapOutputTrackerMasterEndpoint:60 - MapOutputTrackerMasterEndpoint stopped!
2024-09-18 06:01:50 [main] INFO  MemoryStore:60 - MemoryStore cleared
2024-09-18 06:01:50 [main] INFO  BlockManager:60 - BlockManager stopped
2024-09-18 06:01:50 [main] INFO  BlockManagerMaster:60 - BlockManagerMaster stopped
2024-09-18 06:01:50 [dispatcher-event-loop-0] INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:60 - OutputCommitCoordinator stopped!
2024-09-18 06:01:50 [main] INFO  SparkContext:60 - Successfully stopped SparkContext
2024-09-18 06:01:50 [main] INFO  UserActivityETL:57 - spark job finished.
2024-09-18 06:01:50 [shutdown-hook-0] INFO  ShutdownHookManager:60 - Shutdown hook called
2024-09-18 06:01:50 [shutdown-hook-0] INFO  ShutdownHookManager:60 - Deleting directory /tmp/spark-7c8f4194-cd2b-4e40-b088-6b2387869bbc
