/*
 * This source file was generated by the Gradle 'init' task
 */
package user.action;
//spark
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.*;


public class App{

    public static void main(String[] args) {

        // SparkSession 생성 (Hive 지원)
        SparkSession spark = SparkSession.builder()
                .appName("CSV to Hive")
                .master("local[*]") 
                .config("hive.metastore.uris", "thrift://localhost:9083")
                .config("spark.sql.warehouse.dir", "hdfs://localhost:9000/user/hive/warehouse")  // Hive 메타스토어 디렉토리
                .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000")  // HDFS 주소 수정
                .enableHiveSupport()  // Hive 지원 활성화
                .getOrCreate();

        //check hive connect
        Dataset<Row> tables = spark.sql("SHOW TABLES");
        tables.show();

        // 파일 읽기
        String filePath = "file:///Users/hyunsoo/Desktop/dev-study/user-action/2019-Nov.csv.gz";
        Dataset<Row> df = spark.read()
                .format("csv")
                .option("header", "true")
                .option("inferSchema", "true")  // 스키마 자동 유추
                .load(filePath); 

        // UTC 시간을 KST로 변환 (컬럼 이름을 utc_time_column으로 가정)
        Dataset<Row> dfWithKST = df.withColumn("kst_time", 
            from_unixtime(unix_timestamp(col("event_time"), "yyyy-MM-dd HH:mm:ss").plus(9 * 3600))); 

        dfWithKST.show(1);

        //KST 날짜를 기반으로 파티션 컬럼 추가
        Dataset<Row> dfWithPartition = dfWithKST.withColumn("partition_date", 
            date_format(col("kst_time"), "yyyy-MM-dd"));

        // Hive에 저장 (KST 기준 일별 파티션 적용)
        dfWithPartition.write()
            .mode(SaveMode.Append)
            .partitionBy("partition_date")
            .saveAsTable("your_hive_table");

        // Spark 세션 종료
        spark.stop();
    } 
    
}
